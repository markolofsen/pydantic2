{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Pydantic2","text":"<p>Pydantic2 is a powerful library for working with Large Language Models (LLMs) that provides structured responses using Pydantic models. It simplifies the process of interacting with LLMs while ensuring type safety and data validation.</p>"},{"location":"#features","title":"Features","text":""},{"location":"#structured-responses","title":"\ud83d\ude80 Structured Responses","text":"<ul> <li>Define response structures using Pydantic models</li> <li>Automatic validation and parsing of LLM responses</li> <li>Type safety and IDE support</li> </ul>"},{"location":"#usage-tracking","title":"\ud83d\udcca Usage Tracking","text":"<ul> <li>Built-in usage statistics</li> <li>Cost tracking and budgeting</li> <li>Detailed request history</li> </ul>"},{"location":"#agent-system","title":"\ud83e\udd16 Agent System","text":"<ul> <li>Simple yet powerful agent framework</li> <li>Built-in tools for common tasks</li> <li>Easy to extend with custom tools</li> </ul>"},{"location":"#framework-integration","title":"\ud83d\udd0c Framework Integration","text":"<ul> <li>Seamless integration with FastAPI</li> <li>Django integration support</li> <li>Easy to integrate with other frameworks</li> </ul>"},{"location":"#enterprise-ready","title":"\ud83d\udcbc Enterprise Ready","text":"<ul> <li>Production-grade performance</li> <li>Comprehensive security features</li> <li>Detailed documentation</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install pydantic2\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>from pydantic import BaseModel, Field\nfrom typing import List\n\nclass MovieReview(BaseModel):\n    title: str = Field(description=\"The title of the movie\")\n    rating: float = Field(description=\"The rating of the movie\")\n    pros: List[str] = Field(description=\"The pros of the movie\")\n    cons: List[str] = Field(description=\"The cons of the movie\")\n\nclient = LiteLLMClient(Request(\n    model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",\n    answer_model=MovieReview\n))\n\nresponse = client.generate_response(\n    prompt=\"Review the movie 'Inception'\"\n)\n\nprint(f\"Title: {response.title}\")\nprint(f\"Rating: {response.rating}/5\")\nprint(\"Pros:\", \", \".join(response.pros))\nprint(\"Cons:\", \", \".join(response.cons))\n</code></pre>"},{"location":"#why-pydantic2","title":"Why Pydantic2?","text":"<ul> <li>Type Safety: Get structured responses with proper type hints and validation</li> <li>Efficiency: Reduce boilerplate code and focus on your application logic</li> <li>Reliability: Production-tested with comprehensive error handling</li> <li>Flexibility: Support for multiple LLM providers and frameworks</li> <li>Scalability: Built for both small projects and enterprise applications</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation Guide</li> <li>Quick Start Guide</li> <li>Configuration Guide</li> </ul>"},{"location":"#examples","title":"Examples","text":"<ul> <li>Basic Usage</li> <li>Django Integration</li> <li>FastAPI Integration</li> <li>Agent System</li> </ul>"},{"location":"#community","title":"Community","text":"<ul> <li>GitHub Repository</li> <li>Issue Tracker</li> <li>Contributing Guide</li> </ul>"},{"location":"#support","title":"Support","text":"<p>For support and questions: - Email: info@unrealos.com - GitHub Discussions: Pydantic2 Discussions</p>"},{"location":"#license","title":"License","text":"<p>Pydantic2 is released under the MIT License. See the License page for more details.</p>"},{"location":"about/changelog/","title":"Changelog","text":"<p>All notable changes to Pydantic2 will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"about/changelog/#100-2024-03-15","title":"[1.0.0] - 2024-03-15","text":""},{"location":"about/changelog/#added","title":"Added","text":"<ul> <li>Initial release of Pydantic2</li> <li>Core client functionality for interacting with LLMs</li> <li>Structured response parsing with Pydantic models</li> <li>Basic usage tracking and statistics</li> <li>Simple agent system with tool support</li> <li>Integration with FastAPI and Django</li> <li>Comprehensive documentation</li> <li>Basic examples and guides</li> <li>Unit tests and integration tests</li> </ul>"},{"location":"about/changelog/#changed","title":"Changed","text":"<ul> <li>N/A (Initial release)</li> </ul>"},{"location":"about/changelog/#deprecated","title":"Deprecated","text":"<ul> <li>N/A (Initial release)</li> </ul>"},{"location":"about/changelog/#removed","title":"Removed","text":"<ul> <li>N/A (Initial release)</li> </ul>"},{"location":"about/changelog/#fixed","title":"Fixed","text":"<ul> <li>N/A (Initial release)</li> </ul>"},{"location":"about/changelog/#security","title":"Security","text":"<ul> <li>Initial security measures implemented</li> <li>Basic rate limiting</li> <li>Token validation</li> <li>API key management</li> </ul>"},{"location":"about/changelog/#090-2024-03-01","title":"[0.9.0] - 2024-03-01","text":""},{"location":"about/changelog/#added_1","title":"Added","text":"<ul> <li>Beta release of core functionality</li> <li>Initial implementation of client</li> <li>Basic model support</li> <li>Preliminary documentation</li> </ul>"},{"location":"about/changelog/#changed_1","title":"Changed","text":"<ul> <li>Improved error handling</li> <li>Enhanced response validation</li> </ul>"},{"location":"about/changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Various bug fixes and improvements</li> </ul>"},{"location":"about/changelog/#080-2024-02-15","title":"[0.8.0] - 2024-02-15","text":""},{"location":"about/changelog/#added_2","title":"Added","text":"<ul> <li>Alpha release with basic functionality</li> <li>Initial codebase setup</li> <li>Basic project structure</li> </ul>"},{"location":"about/changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"about/changelog/#added_3","title":"Added","text":"<ul> <li>Enhanced response validation</li> <li>Advanced usage tracking features</li> <li>Improved agent system capabilities</li> <li>Additional framework integrations</li> </ul>"},{"location":"about/changelog/#changed_2","title":"Changed","text":"<ul> <li>Performance optimizations</li> <li>Documentation improvements</li> </ul>"},{"location":"about/changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Various bug fixes and improvements</li> </ul>"},{"location":"about/changelog/#how-to-update","title":"How to Update","text":"<p>To update to the latest version of Pydantic2:</p> <pre><code>pip install --upgrade pydantic2\n</code></pre> <p>For major version updates, please refer to the migration guide for any breaking changes and migration instructions.</p>"},{"location":"about/contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to Pydantic2! This document provides guidelines and instructions for contributing to the project.</p>"},{"location":"about/contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Please read and follow our Code of Conduct.</p>"},{"location":"about/contributing/#how-to-contribute","title":"How to Contribute","text":"<p>There are many ways to contribute to Pydantic2:</p> <ol> <li>Report bugs: If you find a bug, please report it by creating an issue on GitHub.</li> <li>Suggest features: If you have an idea for a new feature, please create an issue on GitHub.</li> <li>Improve documentation: If you find a mistake in the documentation or think something could be explained better, please create a pull request.</li> <li>Write code: If you want to contribute code, please create a pull request.</li> </ol>"},{"location":"about/contributing/#development-setup","title":"Development Setup","text":"<p>To set up a development environment:</p> <ol> <li>Fork the repository on GitHub.</li> <li>Clone your fork:    <pre><code>git clone https://github.com/markolofsen/pydantic2.git\ncd pydantic2\n</code></pre></li> <li>Install the package in development mode:    <pre><code>pip install -e \".[dev]\"\n</code></pre></li> <li>Install pre-commit hooks:    <pre><code>pre-commit install\n</code></pre></li> </ol>"},{"location":"about/contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Create a new branch for your changes:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></li> <li>Make your changes.</li> <li>Run the tests:    <pre><code>pytest\n</code></pre></li> <li>Run the linters:    <pre><code>black .\nflake8\nisort .\n</code></pre></li> <li>Commit your changes:    <pre><code>git commit -m \"Add your feature\"\n</code></pre></li> <li>Push your changes to your fork:    <pre><code>git push origin feature/your-feature-name\n</code></pre></li> <li>Create a pull request on GitHub.</li> </ol>"},{"location":"about/contributing/#coding-standards","title":"Coding Standards","text":"<ul> <li>Follow PEP 8 for code style.</li> <li>Use Black for code formatting.</li> <li>Use isort for import sorting.</li> <li>Use flake8 for linting.</li> <li>Write docstrings for all functions, classes, and methods.</li> <li>Write tests for all new features.</li> </ul>"},{"location":"about/contributing/#testing","title":"Testing","text":"<p>We use pytest for testing. To run the tests:</p> <pre><code>pytest\n</code></pre> <p>To run the tests with coverage:</p> <pre><code>pytest --cov=src/pydantic2\n</code></pre>"},{"location":"about/contributing/#documentation","title":"Documentation","text":"<p>We use MkDocs for documentation. To build the documentation:</p> <pre><code>mkdocs build\n</code></pre> <p>To serve the documentation locally:</p> <pre><code>mkdocs serve\n</code></pre>"},{"location":"about/contributing/#release-process","title":"Release Process","text":"<ol> <li>Update the version number in <code>setup.cfg</code>.</li> <li>Update the changelog.</li> <li>Create a new release on GitHub.</li> <li>Build and upload the package to PyPI:    <pre><code>python -m build\ntwine upload dist/*\n</code></pre></li> </ol>"},{"location":"about/contributing/#contact","title":"Contact","text":"<p>If you have any questions, please contact info@unrealos.com.</p>"},{"location":"about/license/","title":"License","text":"<p>Pydantic2 is licensed under the MIT License.</p>"},{"location":"about/license/#mit-license","title":"MIT License","text":"<pre><code>MIT License\n\nCopyright (c) 2024 Unrealos Inc.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"about/license/#third-party-licenses","title":"Third-Party Licenses","text":"<p>Pydantic2 uses several third-party libraries, each with its own license:</p>"},{"location":"about/license/#litellm","title":"LiteLLM","text":"<p>LiteLLM is licensed under the MIT License.</p>"},{"location":"about/license/#pydantic","title":"Pydantic","text":"<p>Pydantic is licensed under the MIT License.</p>"},{"location":"about/license/#instructor","title":"Instructor","text":"<p>Instructor is licensed under the MIT License.</p>"},{"location":"about/license/#smolagents","title":"SmoLAgents","text":"<p>SmoLAgents is licensed under the MIT License.</p>"},{"location":"about/license/#drf-pydantic","title":"DRF-Pydantic","text":"<p>DRF-Pydantic is licensed under the MIT License.</p>"},{"location":"about/license/#contact","title":"Contact","text":"<p>If you have any questions about the license, please contact info@unrealos.com.</p>"},{"location":"about/roadmap/","title":"Roadmap","text":"<p>This document outlines the planned development roadmap for Pydantic2. Please note that this roadmap is subject to change based on community feedback and project priorities.</p>"},{"location":"about/roadmap/#current-version-10x","title":"Current Version (1.0.x)","text":"<p>The current version of Pydantic2 focuses on providing a solid foundation for structured LLM responses with:</p> <ul> <li>Core client functionality for interacting with LLMs</li> <li>Structured response parsing with Pydantic models</li> <li>Basic usage tracking and statistics</li> <li>Simple agent system with tool support</li> <li>Integration with FastAPI and Django</li> </ul>"},{"location":"about/roadmap/#short-term-goals-next-3-6-months","title":"Short-term Goals (Next 3-6 Months)","text":""},{"location":"about/roadmap/#enhanced-response-validation","title":"Enhanced Response Validation","text":"<ul> <li>Improved validation of LLM responses against Pydantic models</li> <li>Better error handling and recovery for malformed responses</li> <li>Support for more complex nested models and validation rules</li> </ul>"},{"location":"about/roadmap/#advanced-usage-tracking","title":"Advanced Usage Tracking","text":"<ul> <li>More detailed usage analytics and reporting</li> <li>Enhanced visualization of usage patterns</li> <li>Export capabilities for usage data</li> </ul>"},{"location":"about/roadmap/#agent-system-improvements","title":"Agent System Improvements","text":"<ul> <li>More built-in tools for common tasks</li> <li>Support for multi-agent collaboration</li> <li>Memory and context management for agents</li> </ul>"},{"location":"about/roadmap/#performance-optimizations","title":"Performance Optimizations","text":"<ul> <li>Reduced latency for high-volume applications</li> <li>Optimized token counting and cost calculation</li> <li>Caching mechanisms for frequently used responses</li> </ul>"},{"location":"about/roadmap/#medium-term-goals-6-12-months","title":"Medium-term Goals (6-12 Months)","text":""},{"location":"about/roadmap/#advanced-model-management","title":"Advanced Model Management","text":"<ul> <li>Support for fine-tuned models</li> <li>Model performance comparison tools</li> <li>Automated model selection based on task requirements</li> </ul>"},{"location":"about/roadmap/#enterprise-features","title":"Enterprise Features","text":"<ul> <li>Role-based access control</li> <li>Multi-tenant support</li> <li>Advanced budget management and alerts</li> </ul>"},{"location":"about/roadmap/#expanded-framework-integrations","title":"Expanded Framework Integrations","text":"<ul> <li>Flask integration</li> <li>Streamlit integration</li> <li>Jupyter notebook extensions</li> </ul>"},{"location":"about/roadmap/#developer-experience","title":"Developer Experience","text":"<ul> <li>Improved documentation with more examples</li> <li>CLI tools for common tasks</li> <li>Visual debugging tools for response parsing</li> </ul>"},{"location":"about/roadmap/#long-term-vision-beyond-12-months","title":"Long-term Vision (Beyond 12 Months)","text":""},{"location":"about/roadmap/#ai-orchestration","title":"AI Orchestration","text":"<ul> <li>Complex workflows with multiple LLM calls</li> <li>Automated prompt optimization</li> <li>Hybrid systems combining multiple AI models</li> </ul>"},{"location":"about/roadmap/#specialized-vertical-solutions","title":"Specialized Vertical Solutions","text":"<ul> <li>Domain-specific extensions for legal, medical, financial applications</li> <li>Industry-specific response models and validation rules</li> </ul>"},{"location":"about/roadmap/#community-and-ecosystem","title":"Community and Ecosystem","text":"<ul> <li>Plugin system for community extensions</li> <li>Marketplace for sharing response models</li> <li>Integration with popular AI development platforms</li> </ul>"},{"location":"about/roadmap/#how-to-influence-the-roadmap","title":"How to Influence the Roadmap","text":"<p>We welcome community input on our roadmap! To suggest features or changes:</p> <ol> <li>Open an issue on GitHub with the \"enhancement\" label</li> <li>Join our community discussions</li> <li>Contribute code or documentation that aligns with the roadmap</li> </ol> <p>For major feature requests, please provide: - A clear description of the feature - Use cases and benefits - Any implementation ideas you may have</p>"},{"location":"about/roadmap/#experimental-features","title":"Experimental Features","text":"<p>We maintain several experimental features that may be promoted to core functionality based on user feedback:</p> <ul> <li>Streaming response parsing</li> <li>Automatic prompt generation from models</li> <li>Response quality scoring</li> </ul> <p>To try experimental features, check the documentation for the latest \"experimental\" package.</p>"},{"location":"about/security/","title":"Security","text":"<p>At Pydantic2, we take security seriously. This document outlines our security policies and provides guidance for reporting security issues.</p>"},{"location":"about/security/#reporting-a-vulnerability","title":"Reporting a Vulnerability","text":"<p>If you discover a security vulnerability in Pydantic2, please follow these steps:</p> <ol> <li>DO NOT create a public GitHub issue.</li> <li>Email us at security@unrealos.com with:</li> <li>A description of the vulnerability</li> <li>Steps to reproduce</li> <li>Potential impact</li> <li>Any suggested fixes (if applicable)</li> <li>You will receive a response within 48 hours acknowledging receipt of your report.</li> <li>We will work with you to understand and address the issue.</li> </ol>"},{"location":"about/security/#security-features","title":"Security Features","text":""},{"location":"about/security/#api-key-management","title":"API Key Management","text":"<ul> <li>Secure storage of API keys</li> <li>Support for environment variables</li> <li>Key rotation capabilities</li> <li>Access control and permissions</li> </ul>"},{"location":"about/security/#rate-limiting","title":"Rate Limiting","text":"<ul> <li>Built-in rate limiting for API calls</li> <li>Configurable limits per client</li> <li>Protection against abuse</li> <li>Monitoring and alerts</li> </ul>"},{"location":"about/security/#data-protection","title":"Data Protection","text":"<ul> <li>Secure handling of sensitive data</li> <li>Encryption of stored credentials</li> <li>Safe logging practices</li> <li>Memory security</li> </ul>"},{"location":"about/security/#input-validation","title":"Input Validation","text":"<ul> <li>Strict validation of all inputs</li> <li>Protection against injection attacks</li> <li>Safe deserialization</li> <li>Type checking and validation</li> </ul>"},{"location":"about/security/#best-practices","title":"Best Practices","text":"<p>When using Pydantic2 in production:</p> <ol> <li>API Keys</li> <li>Never hardcode API keys in your code</li> <li>Use environment variables or secure key management</li> <li>Rotate keys regularly</li> <li> <p>Use different keys for development and production</p> </li> <li> <p>Rate Limiting</p> </li> <li>Configure appropriate rate limits</li> <li>Monitor usage patterns</li> <li>Set up alerts for unusual activity</li> <li> <p>Implement retry mechanisms with backoff</p> </li> <li> <p>Error Handling</p> </li> <li>Never expose sensitive information in error messages</li> <li>Log errors securely</li> <li>Implement proper exception handling</li> <li> <p>Use debug mode only in development</p> </li> <li> <p>Updates</p> </li> <li>Keep Pydantic2 updated to the latest version</li> <li>Monitor security announcements</li> <li>Review changelog for security-related updates</li> <li>Test updates in staging before production</li> </ol>"},{"location":"about/security/#security-audits","title":"Security Audits","text":"<p>We regularly conduct security audits of our codebase. These include:</p> <ul> <li>Static code analysis</li> <li>Dependency scanning</li> <li>Penetration testing</li> <li>Code reviews</li> </ul>"},{"location":"about/security/#responsible-disclosure","title":"Responsible Disclosure","text":"<p>We follow responsible disclosure practices:</p> <ol> <li>Report received and acknowledged within 48 hours</li> <li>Initial assessment completed within 7 days</li> <li>Regular updates on progress</li> <li>Public disclosure after fix is available</li> <li>Credit given to discoverer (if desired)</li> </ol>"},{"location":"about/security/#security-checklist","title":"Security Checklist","text":"<p>When deploying Pydantic2:</p> <ul> <li> Use latest stable version</li> <li> Configure secure API key storage</li> <li> Set appropriate rate limits</li> <li> Enable logging and monitoring</li> <li> Review security documentation</li> <li> Test in staging environment</li> <li> Set up alerts and notifications</li> <li> Plan for regular updates</li> </ul>"},{"location":"about/security/#contact","title":"Contact","text":"<p>For security-related inquiries: - Email: security@unrealos.com - Response time: Within 48 hours - Working hours: Monday-Friday, 9:00-17:00 UTC</p> <p>For general security questions: - Email: info@unrealos.com - GitHub Discussions: Pydantic2 Security Discussions</p>"},{"location":"about/team/","title":"Team","text":"<p>Pydantic2 is developed and maintained by a dedicated team of developers and contributors.</p>"},{"location":"about/team/#core-team","title":"Core Team","text":""},{"location":"about/team/#mark-olofsen","title":"Mark Olofsen","text":"<p>Founder &amp; Lead Developer</p> <p>Mark is the creator of Pydantic2 and leads the overall development and direction of the project. With extensive experience in AI and software development, Mark founded Pydantic2 to simplify working with LLMs in production environments.</p> <ul> <li>GitHub: @markolofsen</li> <li>Email: mark@unrealos.com</li> </ul>"},{"location":"about/team/#unrealos-team","title":"UnrealOS Team","text":"<p>Development &amp; Support</p> <p>The UnrealOS team provides development support, testing, and maintenance for the Pydantic2 project.</p> <ul> <li>Website: https://unrealos.com</li> <li>Email: info@unrealos.com</li> </ul>"},{"location":"about/team/#contributors","title":"Contributors","text":"<p>We're grateful to all the contributors who have helped make Pydantic2 better:</p> <ul> <li>List of contributors</li> </ul>"},{"location":"about/team/#acknowledgements","title":"Acknowledgements","text":"<p>Pydantic2 builds upon several excellent open-source projects:</p> <ul> <li>Pydantic - Data validation and settings management using Python type hints</li> <li>LiteLLM - Call all LLM APIs using the same format</li> <li>Instructor - Structured outputs for LLMs</li> <li>FastAPI - Modern, fast web framework for building APIs</li> </ul>"},{"location":"about/team/#join-the-team","title":"Join the Team","text":"<p>We're always looking for contributors to help improve Pydantic2. If you're interested in contributing, please check out our Contributing Guide to get started.</p>"},{"location":"about/team/#contact","title":"Contact","text":"<p>For general inquiries, please contact us at info@unrealos.com.</p> <p>For security issues, please email security@unrealos.com.</p>"},{"location":"api/agents/","title":"Agents API Reference","text":"<p>This page provides detailed information about the agents API in Pydantic2.</p>"},{"location":"api/agents/#simpleagent","title":"SimpleAgent","text":"<p>The <code>SimpleAgent</code> class is the main class for creating AI agents with specific capabilities.</p>"},{"location":"api/agents/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    model_id: str = \"openrouter/openai/gpt-4o-mini\",\n    response_model: Type[BaseModel] = None,\n    temperature: float = 0.7,\n    max_tokens: int = 1000,\n    verbose: bool = False\n):\n    \"\"\"Initialize the SimpleAgent with configuration parameters.\"\"\"\n</code></pre>"},{"location":"api/agents/#parameters","title":"Parameters","text":"<ul> <li><code>model_id</code> (<code>str</code>): The model identifier. Default: <code>\"openrouter/openai/gpt-4o-mini\"</code>.</li> <li><code>response_model</code> (<code>Type[BaseModel]</code>, optional): The Pydantic model for responses. If not provided, a default model will be used.</li> <li><code>temperature</code> (<code>float</code>): The temperature for response randomness (0.0-1.0). Default: <code>0.7</code>.</li> <li><code>max_tokens</code> (<code>int</code>): The maximum number of tokens in the response. Default: <code>1000</code>.</li> <li><code>verbose</code> (<code>bool</code>): Whether to show detailed output. Default: <code>False</code>.</li> </ul>"},{"location":"api/agents/#methods","title":"Methods","text":""},{"location":"api/agents/#add_tools","title":"add_tools","text":"<pre><code>def add_tools(self, tools: List[Callable]) -&gt; None:\n    \"\"\"Add tools to the agent.\"\"\"\n</code></pre> <p>Adds tools to the agent.</p> <p>Parameters: - <code>tools</code> (<code>List[Callable]</code>): A list of tool functions.</p> <p>Example: <pre><code>agent.add_tools([get_time, process_text])\n</code></pre></p>"},{"location":"api/agents/#run","title":"run","text":"<pre><code>def run(self, query: str) -&gt; Any:\n    \"\"\"Run the agent with a query.\"\"\"\n</code></pre> <p>Runs the agent with a query.</p> <p>Parameters: - <code>query</code> (<code>str</code>): The query to run the agent with.</p> <p>Returns: - An instance of the response model.</p> <p>Example: <pre><code>result = agent.run(\"What time is it?\")\n</code></pre></p>"},{"location":"api/agents/#launch_gradio_ui","title":"launch_gradio_ui","text":"<pre><code>def launch_gradio_ui(\n    self,\n    title: str = \"Pydantic2 Agent\",\n    description: str = \"Ask a question and the agent will answer using its tools.\",\n    theme: str = \"default\",\n    share: bool = False\n) -&gt; None:\n    \"\"\"Launch a Gradio UI for the agent.\"\"\"\n</code></pre> <p>Launches a Gradio UI for the agent.</p> <p>Parameters: - <code>title</code> (<code>str</code>): The title of the UI. Default: <code>\"Pydantic2 Agent\"</code>. - <code>description</code> (<code>str</code>): The description of the UI. Default: <code>\"Ask a question and the agent will answer using its tools.\"</code>. - <code>theme</code> (<code>str</code>): The theme of the UI. Default: <code>\"default\"</code>. - <code>share</code> (<code>bool</code>): Whether to share the UI. Default: <code>False</code>.</p> <p>Example: <pre><code>agent.launch_gradio_ui(\n    title=\"My Agent\",\n    description=\"Ask me anything!\",\n    theme=\"default\",\n    share=True\n)\n</code></pre></p>"},{"location":"api/agents/#tool","title":"tool","text":"<p>The <code>tool</code> decorator is used to define tools for the agent.</p> <pre><code>def tool(name: str, description: str) -&gt; Callable:\n    \"\"\"Decorator for defining a tool.\"\"\"\n</code></pre>"},{"location":"api/agents/#parameters_1","title":"Parameters","text":"<ul> <li><code>name</code> (<code>str</code>): The name of the tool.</li> <li><code>description</code> (<code>str</code>): The description of the tool.</li> </ul> <p>Returns: - A decorator function.</p> <p>Example: <pre><code>@tool(\"get_time\", \"Get the current time\")\ndef get_current_time() -&gt; str:\n    \"\"\"Get the current time.\"\"\"\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n</code></pre></p>"},{"location":"api/agents/#response-models","title":"Response Models","text":""},{"location":"api/agents/#agentresponse","title":"AgentResponse","text":"<p>The <code>AgentResponse</code> class is the default response model for the agent.</p> <pre><code>class AgentResponse(BaseModel):\n    \"\"\"Default response model for the agent.\"\"\"\n    answer: str = Field(..., description=\"The main answer\")\n    reasoning: str = Field(..., description=\"The reasoning process\")\n    tools_used: List[str] = Field(default_factory=list, description=\"List of tools used\")\n</code></pre>"},{"location":"api/agents/#fields","title":"Fields","text":"<ul> <li><code>answer</code> (<code>str</code>): The main answer.</li> <li><code>reasoning</code> (<code>str</code>): The reasoning process.</li> <li><code>tools_used</code> (<code>List[str]</code>): A list of tools used.</li> </ul>"},{"location":"api/agents/#custom-response-models","title":"Custom Response Models","text":"<p>You can define custom response models for the agent:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List, Optional\n\nclass WeatherInfo(BaseModel):\n    temperature: float = Field(..., description=\"Temperature in Celsius\")\n    conditions: str = Field(..., description=\"Weather conditions\")\n    humidity: float = Field(..., description=\"Humidity percentage\")\n    wind_speed: float = Field(..., description=\"Wind speed in km/h\")\n\nclass CustomAgentResponse(BaseModel):\n    answer: str = Field(..., description=\"The main answer\")\n    reasoning: str = Field(..., description=\"The reasoning process\")\n    tools_used: List[str] = Field(default_factory=list, description=\"List of tools used\")\n    weather_info: Optional[WeatherInfo] = Field(None, description=\"Weather information if requested\")\n</code></pre>"},{"location":"api/agents/#complete-example","title":"Complete Example","text":"<p>Here's a complete example of using the agents API:</p> <pre><code>from pydantic2.agents import SimpleAgent, tool\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\nimport datetime\nimport random\n\nclass WeatherInfo(BaseModel):\n    temperature: float = Field(..., description=\"Temperature in Celsius\")\n    conditions: str = Field(..., description=\"Weather conditions\")\n    humidity: float = Field(..., description=\"Humidity percentage\")\n    wind_speed: float = Field(..., description=\"Wind speed in km/h\")\n\nclass AgentResponse(BaseModel):\n    answer: str = Field(..., description=\"The main answer\")\n    reasoning: str = Field(..., description=\"The reasoning process\")\n    tools_used: List[str] = Field(default_factory=list)\n    weather_info: Optional[WeatherInfo] = Field(None, description=\"Weather information if requested\")\n\n@tool(\"get_time\", \"Get the current time\")\ndef get_current_time() -&gt; str:\n    \"\"\"Get the current time.\"\"\"\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n@tool(\"get_weather\", \"Get the current weather for a location\")\ndef get_weather(location: str) -&gt; dict:\n    \"\"\"Get the current weather for a location.\"\"\"\n    # This is a mock implementation\n    weather_data = {\n        \"New York\": {\n            \"temperature\": 22.5,\n            \"conditions\": \"Partly cloudy\",\n            \"humidity\": 65.0,\n            \"wind_speed\": 10.2\n        },\n        \"London\": {\n            \"temperature\": 18.0,\n            \"conditions\": \"Rainy\",\n            \"humidity\": 80.0,\n            \"wind_speed\": 15.5\n        },\n        \"Tokyo\": {\n            \"temperature\": 28.0,\n            \"conditions\": \"Sunny\",\n            \"humidity\": 70.0,\n            \"wind_speed\": 8.0\n        }\n    }\n\n    return weather_data.get(location, {\n        \"temperature\": 20.0,\n        \"conditions\": \"Unknown\",\n        \"humidity\": 50.0,\n        \"wind_speed\": 5.0\n    })\n\n@tool(\"roll_dice\", \"Roll a dice with the specified number of sides\")\ndef roll_dice(sides: int = 6) -&gt; int:\n    \"\"\"Roll a dice with the specified number of sides.\"\"\"\n    return random.randint(1, sides)\n\n# Create and configure the agent\nagent = SimpleAgent(\n    model_id=\"openrouter/openai/gpt-4o-mini\",\n    response_model=AgentResponse\n)\n\n# Add tools\nagent.add_tools([\n    get_time,\n    get_weather,\n    roll_dice\n])\n\n# Run the agent\nresult = agent.run(\"What's the weather like in Tokyo?\")\nprint(result)\n\n# Launch Gradio UI\nagent.launch_gradio_ui(\n    title=\"Weather Agent\",\n    description=\"Ask about the weather in different cities.\",\n    theme=\"default\",\n    share=True\n)\n</code></pre>"},{"location":"api/client/","title":"Client API Reference","text":"<p>This page provides detailed information about the client API in Pydantic2.</p>"},{"location":"api/client/#litellmclient","title":"LiteLLMClient","text":"<p>The <code>LiteLLMClient</code> class is the main client for interacting with LLMs.</p>"},{"location":"api/client/#constructor","title":"Constructor","text":"<pre><code>def __init__(self, config: Request):\n    \"\"\"Initialize the LiteLLMClient with a configuration.\"\"\"\n</code></pre>"},{"location":"api/client/#parameters","title":"Parameters","text":"<ul> <li><code>config</code> (<code>Request</code>): The configuration for the client.</li> </ul>"},{"location":"api/client/#properties","title":"Properties","text":"<ul> <li><code>config</code> (<code>Request</code>): The configuration for the client.</li> <li><code>msg</code> (<code>MessageHandler</code>): The message handler for the client.</li> <li><code>meta</code> (<code>Meta</code>): The metadata for the client.</li> <li><code>usage_tracker</code> (<code>UsageClass</code>): The usage tracker for the client.</li> </ul>"},{"location":"api/client/#methods","title":"Methods","text":""},{"location":"api/client/#generate_response","title":"generate_response","text":"<pre><code>def generate_response(self) -&gt; Any:\n    \"\"\"Generate a response from the LLM.\"\"\"\n</code></pre> <p>Generates a response from the LLM based on the messages in the message handler.</p> <p>Returns: - An instance of the response model specified in the configuration.</p> <p>Example: <pre><code>response = client.generate_response()\n</code></pre></p>"},{"location":"api/client/#count_tokens","title":"count_tokens","text":"<pre><code>def count_tokens(self) -&gt; int:\n    \"\"\"Count the number of tokens in the prompt.\"\"\"\n</code></pre> <p>Counts the number of tokens in the prompt.</p> <p>Returns: - The number of tokens in the prompt.</p> <p>Example: <pre><code>token_count = client.count_tokens()\n</code></pre></p>"},{"location":"api/client/#get_max_tokens_for_model","title":"get_max_tokens_for_model","text":"<pre><code>def get_max_tokens_for_model(self) -&gt; int:\n    \"\"\"Get the maximum number of tokens for the model.\"\"\"\n</code></pre> <p>Gets the maximum number of tokens for the model.</p> <p>Returns: - The maximum number of tokens for the model.</p> <p>Example: <pre><code>max_tokens = client.get_max_tokens_for_model()\n</code></pre></p>"},{"location":"api/client/#calculate_cost","title":"calculate_cost","text":"<pre><code>def calculate_cost(self) -&gt; float:\n    \"\"\"Calculate the cost of the request.\"\"\"\n</code></pre> <p>Calculates the cost of the request.</p> <p>Returns: - The cost of the request in USD.</p> <p>Example: <pre><code>cost = client.calculate_cost()\n</code></pre></p>"},{"location":"api/client/#get_budget_info","title":"get_budget_info","text":"<pre><code>def get_budget_info(self) -&gt; dict:\n    \"\"\"Get information about the budget.\"\"\"\n</code></pre> <p>Gets information about the budget.</p> <p>Returns: - A dictionary with budget information.</p> <p>Example: <pre><code>budget_info = client.get_budget_info()\n</code></pre></p>"},{"location":"api/client/#print_usage_info","title":"print_usage_info","text":"<pre><code>def print_usage_info(self) -&gt; None:\n    \"\"\"Print usage information.\"\"\"\n</code></pre> <p>Prints usage information.</p> <p>Example: <pre><code>client.print_usage_info()\n</code></pre></p>"},{"location":"api/client/#request","title":"Request","text":"<p>The <code>Request</code> class represents the configuration for a request to an LLM.</p>"},{"location":"api/client/#constructor_1","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    model: str = \"openrouter/openai/gpt-4o-mini-2024-07-18\",\n    answer_model: Type[BaseModel] = None,\n    temperature: float = 0.7,\n    max_tokens: int = 500,\n    top_p: float = 1.0,\n    frequency_penalty: float = 0.0,\n    presence_penalty: float = 0.0,\n    online: bool = False,\n    cache_prompt: bool = True,\n    max_budget: float = 0.05,\n    timeout: int = 60,\n    user_id: str = None,\n    client_id: str = \"default\",\n    verbose: bool = False,\n    logs: bool = False\n):\n    \"\"\"Initialize the Request with configuration parameters.\"\"\"\n</code></pre>"},{"location":"api/client/#parameters_1","title":"Parameters","text":"<ul> <li><code>model</code> (<code>str</code>): The model identifier. Default: <code>\"openrouter/openai/gpt-4o-mini-2024-07-18\"</code>.</li> <li><code>answer_model</code> (<code>Type[BaseModel]</code>): The Pydantic model for responses. Required.</li> <li><code>temperature</code> (<code>float</code>): The temperature for response randomness (0.0-1.0). Default: <code>0.7</code>.</li> <li><code>max_tokens</code> (<code>int</code>): The maximum number of tokens in the response. Default: <code>500</code>.</li> <li><code>top_p</code> (<code>float</code>): The nucleus sampling parameter. Default: <code>1.0</code>.</li> <li><code>frequency_penalty</code> (<code>float</code>): The penalty for token frequency. Default: <code>0.0</code>.</li> <li><code>presence_penalty</code> (<code>float</code>): The penalty for token presence. Default: <code>0.0</code>.</li> <li><code>online</code> (<code>bool</code>): Whether to enable web search. Default: <code>False</code>.</li> <li><code>cache_prompt</code> (<code>bool</code>): Whether to cache identical prompts. Default: <code>True</code>.</li> <li><code>max_budget</code> (<code>float</code>): The maximum cost per request in USD. Default: <code>0.05</code>.</li> <li><code>timeout</code> (<code>int</code>): The request timeout in seconds. Default: <code>60</code>.</li> <li><code>user_id</code> (<code>str</code>): The user identifier for budget tracking. Default: <code>None</code>.</li> <li><code>client_id</code> (<code>str</code>): The client identifier for usage tracking. Default: <code>\"default\"</code>.</li> <li><code>verbose</code> (<code>bool</code>): Whether to show detailed output. Default: <code>False</code>.</li> <li><code>logs</code> (<code>bool</code>): Whether to enable logging. Default: <code>False</code>.</li> </ul>"},{"location":"api/client/#properties_1","title":"Properties","text":"<ul> <li>All parameters are available as properties.</li> </ul>"},{"location":"api/client/#messagehandler","title":"MessageHandler","text":"<p>The <code>MessageHandler</code> class handles messages for the client.</p>"},{"location":"api/client/#constructor_2","title":"Constructor","text":"<pre><code>def __init__(self, config: Request):\n    \"\"\"Initialize the MessageHandler with a configuration.\"\"\"\n</code></pre>"},{"location":"api/client/#parameters_2","title":"Parameters","text":"<ul> <li><code>config</code> (<code>Request</code>): The configuration for the message handler.</li> </ul>"},{"location":"api/client/#methods_1","title":"Methods","text":""},{"location":"api/client/#add_message_system","title":"add_message_system","text":"<pre><code>def add_message_system(self, content: Any) -&gt; None:\n    \"\"\"Add a system message.\"\"\"\n</code></pre> <p>Adds a system message to the conversation.</p> <p>Parameters: - <code>content</code> (<code>Any</code>): The content of the message.</p> <p>Example: <pre><code>client.msg.add_message_system(\"You are a helpful assistant.\")\n</code></pre></p>"},{"location":"api/client/#add_message_user","title":"add_message_user","text":"<pre><code>def add_message_user(self, content: Any) -&gt; None:\n    \"\"\"Add a user message.\"\"\"\n</code></pre> <p>Adds a user message to the conversation.</p> <p>Parameters: - <code>content</code> (<code>Any</code>): The content of the message.</p> <p>Example: <pre><code>client.msg.add_message_user(\"What is the capital of France?\")\n</code></pre></p>"},{"location":"api/client/#add_message_assistant","title":"add_message_assistant","text":"<pre><code>def add_message_assistant(self, content: Any) -&gt; None:\n    \"\"\"Add an assistant message.\"\"\"\n</code></pre> <p>Adds an assistant message to the conversation.</p> <p>Parameters: - <code>content</code> (<code>Any</code>): The content of the message.</p> <p>Example: <pre><code>client.msg.add_message_assistant(\"The capital of France is Paris.\")\n</code></pre></p>"},{"location":"api/client/#add_message_block","title":"add_message_block","text":"<pre><code>def add_message_block(self, tag: str, content: Any) -&gt; None:\n    \"\"\"Add a block message with a tag.\"\"\"\n</code></pre> <p>Adds a block message with a tag to the conversation.</p> <p>Parameters: - <code>tag</code> (<code>str</code>): The tag for the block. - <code>content</code> (<code>Any</code>): The content of the message.</p> <p>Example: <pre><code>client.msg.add_message_block(\"CODE\", \"def hello(): print('Hello, World!')\")\n</code></pre></p>"},{"location":"api/client/#get_messages","title":"get_messages","text":"<pre><code>def get_messages(self) -&gt; List[dict]:\n    \"\"\"Get all messages in the conversation.\"\"\"\n</code></pre> <p>Gets all messages in the conversation.</p> <p>Returns: - A list of dictionaries representing the messages.</p> <p>Example: <pre><code>messages = client.msg.get_messages()\n</code></pre></p>"},{"location":"api/client/#clear_messages","title":"clear_messages","text":"<pre><code>def clear_messages(self) -&gt; None:\n    \"\"\"Clear all messages in the conversation.\"\"\"\n</code></pre> <p>Clears all messages in the conversation.</p> <p>Example: <pre><code>client.msg.clear_messages()\n</code></pre></p>"},{"location":"api/client/#meta","title":"Meta","text":"<p>The <code>Meta</code> class represents metadata for a request.</p>"},{"location":"api/client/#properties_2","title":"Properties","text":"<ul> <li><code>request_id</code> (<code>str</code>): The ID of the request.</li> <li><code>model_used</code> (<code>str</code>): The model used for the request.</li> <li><code>response_time_seconds</code> (<code>float</code>): The response time in seconds.</li> <li><code>token_count</code> (<code>int</code>): The total number of tokens used.</li> <li><code>input_tokens</code> (<code>int</code>): The number of input tokens.</li> <li><code>output_tokens</code> (<code>int</code>): The number of output tokens.</li> </ul>"},{"location":"api/client/#usageclass","title":"UsageClass","text":"<p>The <code>UsageClass</code> class tracks usage statistics.</p>"},{"location":"api/client/#methods_2","title":"Methods","text":""},{"location":"api/client/#get_usage_stats","title":"get_usage_stats","text":"<pre><code>def get_usage_stats(self) -&gt; dict:\n    \"\"\"Get usage statistics.\"\"\"\n</code></pre> <p>Gets usage statistics.</p> <p>Returns: - A dictionary with usage statistics.</p> <p>Example: <pre><code>usage_stats = client.usage_tracker.get_usage_stats()\n</code></pre></p>"},{"location":"api/client/#get_client_usage_data","title":"get_client_usage_data","text":"<pre><code>def get_client_usage_data(self, client_id: str = None) -&gt; ClientUsageData:\n    \"\"\"Get detailed usage data for a client.\"\"\"\n</code></pre> <p>Gets detailed usage data for a client.</p> <p>Parameters: - <code>client_id</code> (<code>str</code>, optional): The client ID. If not provided, uses the client ID from the configuration.</p> <p>Returns: - A <code>ClientUsageData</code> instance with detailed usage data.</p> <p>Example: <pre><code>usage_data = client.usage_tracker.get_client_usage_data()\n</code></pre></p>"},{"location":"api/client/#get_request_details","title":"get_request_details","text":"<pre><code>def get_request_details(self, request_id: str) -&gt; RequestDetails:\n    \"\"\"Get detailed information about a specific request.\"\"\"\n</code></pre> <p>Gets detailed information about a specific request.</p> <p>Parameters: - <code>request_id</code> (<code>str</code>): The request ID.</p> <p>Returns: - A <code>RequestDetails</code> instance with detailed information about the request.</p> <p>Example: <pre><code>request_details = client.usage_tracker.get_request_details(request_id)\n</code></pre></p>"},{"location":"api/client/#clientusagedata","title":"ClientUsageData","text":"<p>The <code>ClientUsageData</code> class represents detailed usage data for a client.</p>"},{"location":"api/client/#properties_3","title":"Properties","text":"<ul> <li><code>client_id</code> (<code>str</code>): The client ID.</li> <li><code>total_requests</code> (<code>int</code>): The total number of requests.</li> <li><code>successful_requests</code> (<code>int</code>): The number of successful requests.</li> <li><code>failed_requests</code> (<code>int</code>): The number of failed requests.</li> <li><code>total_input_tokens</code> (<code>int</code>): The total number of input tokens.</li> <li><code>total_output_tokens</code> (<code>int</code>): The total number of output tokens.</li> <li><code>total_cost</code> (<code>float</code>): The total cost in USD.</li> <li><code>models_used</code> (<code>List[ModelUsage]</code>): A list of models used.</li> <li><code>recent_requests</code> (<code>List[RequestSummary]</code>): A list of recent requests.</li> </ul>"},{"location":"api/client/#requestdetails","title":"RequestDetails","text":"<p>The <code>RequestDetails</code> class represents detailed information about a specific request.</p>"},{"location":"api/client/#properties_4","title":"Properties","text":"<ul> <li><code>request_id</code> (<code>str</code>): The request ID.</li> <li><code>timestamp</code> (<code>datetime</code>): The timestamp of the request.</li> <li><code>model_name</code> (<code>str</code>): The name of the model.</li> <li><code>model_provider</code> (<code>str</code>): The provider of the model.</li> <li><code>client_id</code> (<code>str</code>): The client ID.</li> <li><code>user_id</code> (<code>str</code>): The user ID.</li> <li><code>config_json</code> (<code>str</code>): The configuration as JSON.</li> <li><code>request_raw</code> (<code>str</code>): The raw request.</li> <li><code>request_json</code> (<code>str</code>): The request as JSON.</li> <li><code>response_json</code> (<code>str</code>): The response as JSON.</li> <li><code>response_raw</code> (<code>str</code>): The raw response.</li> <li><code>input_tokens</code> (<code>int</code>): The number of input tokens.</li> <li><code>output_tokens</code> (<code>int</code>): The number of output tokens.</li> <li><code>input_cost</code> (<code>float</code>): The cost of input tokens.</li> <li><code>output_cost</code> (<code>float</code>): The cost of output tokens.</li> <li><code>total_cost</code> (<code>float</code>): The total cost.</li> <li><code>status</code> (<code>str</code>): The status of the request.</li> <li><code>error_message</code> (<code>str</code>): The error message, if any.</li> </ul>"},{"location":"api/models/","title":"Models API Reference","text":"<p>This page provides detailed information about the models API in Pydantic2.</p>"},{"location":"api/models/#base-models","title":"Base Models","text":""},{"location":"api/models/#request","title":"Request","text":"<p>The <code>Request</code> class represents the configuration for a request to an LLM.</p> <pre><code>class Request(BaseModel):\n    \"\"\"Configuration for a request to an LLM.\"\"\"\n    model: str = \"openrouter/openai/gpt-4o-mini-2024-07-18\"\n    answer_model: Type[BaseModel] = None\n    temperature: float = 0.7\n    max_tokens: int = 500\n    top_p: float = 1.0\n    frequency_penalty: float = 0.0\n    presence_penalty: float = 0.0\n    online: bool = False\n    cache_prompt: bool = True\n    max_budget: float = 0.05\n    timeout: int = 60\n    user_id: str = None\n    client_id: str = \"default\"\n    verbose: bool = False\n    logs: bool = False\n</code></pre>"},{"location":"api/models/#fields","title":"Fields","text":"<ul> <li><code>model</code> (<code>str</code>): The model identifier. Default: <code>\"openrouter/openai/gpt-4o-mini-2024-07-18\"</code>.</li> <li><code>answer_model</code> (<code>Type[BaseModel]</code>): The Pydantic model for responses. Required.</li> <li><code>temperature</code> (<code>float</code>): The temperature for response randomness (0.0-1.0). Default: <code>0.7</code>.</li> <li><code>max_tokens</code> (<code>int</code>): The maximum number of tokens in the response. Default: <code>500</code>.</li> <li><code>top_p</code> (<code>float</code>): The nucleus sampling parameter. Default: <code>1.0</code>.</li> <li><code>frequency_penalty</code> (<code>float</code>): The penalty for token frequency. Default: <code>0.0</code>.</li> <li><code>presence_penalty</code> (<code>float</code>): The penalty for token presence. Default: <code>0.0</code>.</li> <li><code>online</code> (<code>bool</code>): Whether to enable web search. Default: <code>False</code>.</li> <li><code>cache_prompt</code> (<code>bool</code>): Whether to cache identical prompts. Default: <code>True</code>.</li> <li><code>max_budget</code> (<code>float</code>): The maximum cost per request in USD. Default: <code>0.05</code>.</li> <li><code>timeout</code> (<code>int</code>): The request timeout in seconds. Default: <code>60</code>.</li> <li><code>user_id</code> (<code>str</code>): The user identifier for budget tracking. Default: <code>None</code>.</li> <li><code>client_id</code> (<code>str</code>): The client identifier for usage tracking. Default: <code>\"default\"</code>.</li> <li><code>verbose</code> (<code>bool</code>): Whether to show detailed output. Default: <code>False</code>.</li> <li><code>logs</code> (<code>bool</code>): Whether to enable logging. Default: <code>False</code>.</li> </ul>"},{"location":"api/models/#meta","title":"Meta","text":"<p>The <code>Meta</code> class represents metadata for a request.</p> <pre><code>class Meta(BaseModel):\n    \"\"\"Metadata for a request.\"\"\"\n    request_id: str = None\n    model_used: str = None\n    response_time_seconds: float = 0.0\n    token_count: int = 0\n    input_tokens: int = 0\n    output_tokens: int = 0\n</code></pre>"},{"location":"api/models/#fields_1","title":"Fields","text":"<ul> <li><code>request_id</code> (<code>str</code>): The ID of the request.</li> <li><code>model_used</code> (<code>str</code>): The model used for the request.</li> <li><code>response_time_seconds</code> (<code>float</code>): The response time in seconds.</li> <li><code>token_count</code> (<code>int</code>): The total number of tokens used.</li> <li><code>input_tokens</code> (<code>int</code>): The number of input tokens.</li> <li><code>output_tokens</code> (<code>int</code>): The number of output tokens.</li> </ul>"},{"location":"api/models/#fullresponse","title":"FullResponse","text":"<p>The <code>FullResponse</code> class represents a full response from an LLM.</p> <pre><code>class FullResponse(BaseModel):\n    \"\"\"Full response from an LLM.\"\"\"\n    answer: Any\n    meta: Meta\n</code></pre>"},{"location":"api/models/#fields_2","title":"Fields","text":"<ul> <li><code>answer</code> (<code>Any</code>): The answer from the LLM.</li> <li><code>meta</code> (<code>Meta</code>): The metadata for the request.</li> </ul>"},{"location":"api/models/#usage-models","title":"Usage Models","text":""},{"location":"api/models/#modelusage","title":"ModelUsage","text":"<p>The <code>ModelUsage</code> class represents usage statistics for a specific model.</p> <pre><code>class ModelUsage(BaseModel):\n    \"\"\"Usage statistics for a specific model.\"\"\"\n    model_name: str\n    model_provider: str\n    total_input_tokens: int\n    total_output_tokens: int\n    total_cost: float\n    last_used: datetime\n</code></pre>"},{"location":"api/models/#fields_3","title":"Fields","text":"<ul> <li><code>model_name</code> (<code>str</code>): The name of the model.</li> <li><code>model_provider</code> (<code>str</code>): The provider of the model.</li> <li><code>total_input_tokens</code> (<code>int</code>): The total number of input tokens.</li> <li><code>total_output_tokens</code> (<code>int</code>): The total number of output tokens.</li> <li><code>total_cost</code> (<code>float</code>): The total cost in USD.</li> <li><code>last_used</code> (<code>datetime</code>): The last time the model was used.</li> </ul>"},{"location":"api/models/#requestsummary","title":"RequestSummary","text":"<p>The <code>RequestSummary</code> class represents a summary of a request.</p> <pre><code>class RequestSummary(BaseModel):\n    \"\"\"Summary of a request.\"\"\"\n    request_id: str\n    timestamp: datetime\n    model_name: str\n    model_provider: str\n    input_tokens: int\n    output_tokens: int\n    total_cost: float\n    status: str\n    error_message: Optional[str] = None\n</code></pre>"},{"location":"api/models/#fields_4","title":"Fields","text":"<ul> <li><code>request_id</code> (<code>str</code>): The ID of the request.</li> <li><code>timestamp</code> (<code>datetime</code>): The timestamp of the request.</li> <li><code>model_name</code> (<code>str</code>): The name of the model.</li> <li><code>model_provider</code> (<code>str</code>): The provider of the model.</li> <li><code>input_tokens</code> (<code>int</code>): The number of input tokens.</li> <li><code>output_tokens</code> (<code>int</code>): The number of output tokens.</li> <li><code>total_cost</code> (<code>float</code>): The total cost in USD.</li> <li><code>status</code> (<code>str</code>): The status of the request.</li> <li><code>error_message</code> (<code>Optional[str]</code>): The error message, if any.</li> </ul>"},{"location":"api/models/#clientusagedata","title":"ClientUsageData","text":"<p>The <code>ClientUsageData</code> class represents detailed usage data for a client.</p> <pre><code>class ClientUsageData(BaseModel):\n    \"\"\"Detailed usage data for a client.\"\"\"\n    client_id: str\n    total_requests: int\n    successful_requests: int\n    failed_requests: int\n    total_input_tokens: int\n    total_output_tokens: int\n    total_cost: float\n    models_used: List[ModelUsage]\n    recent_requests: List[RequestSummary]\n</code></pre>"},{"location":"api/models/#fields_5","title":"Fields","text":"<ul> <li><code>client_id</code> (<code>str</code>): The client ID.</li> <li><code>total_requests</code> (<code>int</code>): The total number of requests.</li> <li><code>successful_requests</code> (<code>int</code>): The number of successful requests.</li> <li><code>failed_requests</code> (<code>int</code>): The number of failed requests.</li> <li><code>total_input_tokens</code> (<code>int</code>): The total number of input tokens.</li> <li><code>total_output_tokens</code> (<code>int</code>): The total number of output tokens.</li> <li><code>total_cost</code> (<code>float</code>): The total cost in USD.</li> <li><code>models_used</code> (<code>List[ModelUsage]</code>): A list of models used.</li> <li><code>recent_requests</code> (<code>List[RequestSummary]</code>): A list of recent requests.</li> </ul>"},{"location":"api/models/#requestdetails","title":"RequestDetails","text":"<p>The <code>RequestDetails</code> class represents detailed information about a specific request.</p> <pre><code>class RequestDetails(BaseModel):\n    \"\"\"Detailed information about a specific request.\"\"\"\n    request_id: str\n    timestamp: datetime\n    model_name: str\n    model_provider: str\n    client_id: str\n    user_id: str\n    config_json: str\n    request_raw: str\n    request_json: str\n    response_json: str\n    response_raw: str\n    input_tokens: int\n    output_tokens: int\n    input_cost: float\n    output_cost: float\n    total_cost: float\n    status: str\n    error_message: Optional[str] = None\n</code></pre>"},{"location":"api/models/#fields_6","title":"Fields","text":"<ul> <li><code>request_id</code> (<code>str</code>): The request ID.</li> <li><code>timestamp</code> (<code>datetime</code>): The timestamp of the request.</li> <li><code>model_name</code> (<code>str</code>): The name of the model.</li> <li><code>model_provider</code> (<code>str</code>): The provider of the model.</li> <li><code>client_id</code> (<code>str</code>): The client ID.</li> <li><code>user_id</code> (<code>str</code>): The user ID.</li> <li><code>config_json</code> (<code>str</code>): The configuration as JSON.</li> <li><code>request_raw</code> (<code>str</code>): The raw request.</li> <li><code>request_json</code> (<code>str</code>): The request as JSON.</li> <li><code>response_json</code> (<code>str</code>): The response as JSON.</li> <li><code>response_raw</code> (<code>str</code>): The raw response.</li> <li><code>input_tokens</code> (<code>int</code>): The number of input tokens.</li> <li><code>output_tokens</code> (<code>int</code>): The number of output tokens.</li> <li><code>input_cost</code> (<code>float</code>): The cost of input tokens.</li> <li><code>output_cost</code> (<code>float</code>): The cost of output tokens.</li> <li><code>total_cost</code> (<code>float</code>): The total cost.</li> <li><code>status</code> (<code>str</code>): The status of the request.</li> <li><code>error_message</code> (<code>Optional[str]</code>): The error message, if any.</li> </ul>"},{"location":"api/models/#model-information","title":"Model Information","text":""},{"location":"api/models/#modelinfo","title":"ModelInfo","text":"<p>The <code>ModelInfo</code> class represents information about a model.</p> <pre><code>class ModelInfo(BaseModel):\n    \"\"\"Information about a model.\"\"\"\n    id: str\n    name: str\n    provider: str\n    input_cost_per_token: float\n    output_cost_per_token: float\n    max_tokens: int\n</code></pre>"},{"location":"api/models/#fields_7","title":"Fields","text":"<ul> <li><code>id</code> (<code>str</code>): The ID of the model.</li> <li><code>name</code> (<code>str</code>): The name of the model.</li> <li><code>provider</code> (<code>str</code>): The provider of the model.</li> <li><code>input_cost_per_token</code> (<code>float</code>): The cost per input token in USD.</li> <li><code>output_cost_per_token</code> (<code>float</code>): The cost per output token in USD.</li> <li><code>max_tokens</code> (<code>int</code>): The maximum number of tokens the model can handle.</li> </ul>"},{"location":"api/models/#universalmodelgetter","title":"UniversalModelGetter","text":"<p>The <code>UniversalModelGetter</code> class is used to get information about models.</p> <pre><code>class UniversalModelGetter:\n    \"\"\"Class for getting information about models.\"\"\"\n    def get_model_by_id(self, model_id: str) -&gt; ModelInfo:\n        \"\"\"Get information about a model by its ID.\"\"\"\n</code></pre>"},{"location":"api/models/#methods","title":"Methods","text":""},{"location":"api/models/#get_model_by_id","title":"get_model_by_id","text":"<pre><code>def get_model_by_id(self, model_id: str) -&gt; ModelInfo:\n    \"\"\"Get information about a model by its ID.\"\"\"\n</code></pre> <p>Gets information about a model by its ID.</p> <p>Parameters: - <code>model_id</code> (<code>str</code>): The ID of the model.</p> <p>Returns: - A <code>ModelInfo</code> instance with information about the model.</p> <p>Example: <pre><code>from pydantic2.client.prices import UniversalModelGetter\n\ngetter = UniversalModelGetter()\nmodel = getter.get_model_by_id(\"openrouter/openai/gpt-4o-mini-2024-07-18\")\nprint(model)\n</code></pre></p>"},{"location":"api/usage/","title":"Usage API Reference","text":"<p>This page provides detailed information about the usage API in Pydantic2.</p>"},{"location":"api/usage/#usageclass","title":"UsageClass","text":"<p>The <code>UsageClass</code> class is the main class for tracking usage statistics.</p>"},{"location":"api/usage/#constructor","title":"Constructor","text":"<pre><code>def __init__(self, config: Request):\n    \"\"\"Initialize the UsageClass with a configuration.\"\"\"\n</code></pre>"},{"location":"api/usage/#parameters","title":"Parameters","text":"<ul> <li><code>config</code> (<code>Request</code>): The configuration for the usage tracker.</li> </ul>"},{"location":"api/usage/#methods","title":"Methods","text":""},{"location":"api/usage/#get_usage_stats","title":"get_usage_stats","text":"<pre><code>def get_usage_stats(self) -&gt; dict:\n    \"\"\"Get usage statistics.\"\"\"\n</code></pre> <p>Gets usage statistics.</p> <p>Returns: - A dictionary with usage statistics.</p> <p>Example: <pre><code>usage_stats = client.usage_tracker.get_usage_stats()\n</code></pre></p>"},{"location":"api/usage/#get_client_usage_data","title":"get_client_usage_data","text":"<pre><code>def get_client_usage_data(self, client_id: str = None) -&gt; ClientUsageData:\n    \"\"\"Get detailed usage data for a client.\"\"\"\n</code></pre> <p>Gets detailed usage data for a client.</p> <p>Parameters: - <code>client_id</code> (<code>str</code>, optional): The client ID. If not provided, uses the client ID from the configuration.</p> <p>Returns: - A <code>ClientUsageData</code> instance with detailed usage data.</p> <p>Example: <pre><code>usage_data = client.usage_tracker.get_client_usage_data()\n</code></pre></p>"},{"location":"api/usage/#get_request_details","title":"get_request_details","text":"<pre><code>def get_request_details(self, request_id: str) -&gt; RequestDetails:\n    \"\"\"Get detailed information about a specific request.\"\"\"\n</code></pre> <p>Gets detailed information about a specific request.</p> <p>Parameters: - <code>request_id</code> (<code>str</code>): The request ID.</p> <p>Returns: - A <code>RequestDetails</code> instance with detailed information about the request.</p> <p>Example: <pre><code>request_details = client.usage_tracker.get_request_details(request_id)\n</code></pre></p>"},{"location":"api/usage/#log_request","title":"log_request","text":"<pre><code>def log_request(\n    self,\n    request_id: str,\n    model_name: str,\n    model_provider: str,\n    client_id: str,\n    user_id: str,\n    config_json: str,\n    request_raw: str,\n    request_json: str,\n    response_json: str,\n    response_raw: str,\n    input_tokens: int,\n    output_tokens: int,\n    input_cost: float,\n    output_cost: float,\n    status: str,\n    error_message: str = None\n) -&gt; None:\n    \"\"\"Log a request.\"\"\"\n</code></pre> <p>Logs a request.</p> <p>Parameters: - <code>request_id</code> (<code>str</code>): The ID of the request. - <code>model_name</code> (<code>str</code>): The name of the model. - <code>model_provider</code> (<code>str</code>): The provider of the model. - <code>client_id</code> (<code>str</code>): The client ID. - <code>user_id</code> (<code>str</code>): The user ID. - <code>config_json</code> (<code>str</code>): The configuration as JSON. - <code>request_raw</code> (<code>str</code>): The raw request. - <code>request_json</code> (<code>str</code>): The request as JSON. - <code>response_json</code> (<code>str</code>): The response as JSON. - <code>response_raw</code> (<code>str</code>): The raw response. - <code>input_tokens</code> (<code>int</code>): The number of input tokens. - <code>output_tokens</code> (<code>int</code>): The number of output tokens. - <code>input_cost</code> (<code>float</code>): The cost of input tokens. - <code>output_cost</code> (<code>float</code>): The cost of output tokens. - <code>status</code> (<code>str</code>): The status of the request. - <code>error_message</code> (<code>str</code>, optional): The error message, if any.</p> <p>Example: <pre><code>client.usage_tracker.log_request(\n    request_id=\"123\",\n    model_name=\"gpt-4\",\n    model_provider=\"openai\",\n    client_id=\"my_app\",\n    user_id=\"user123\",\n    config_json=\"{}\",\n    request_raw=\"\",\n    request_json=\"{}\",\n    response_json=\"{}\",\n    response_raw=\"\",\n    input_tokens=10,\n    output_tokens=20,\n    input_cost=0.0001,\n    output_cost=0.0002,\n    status=\"success\"\n)\n</code></pre></p>"},{"location":"api/usage/#usagelogger","title":"UsageLogger","text":"<p>The <code>UsageLogger</code> class is a simpler class for logging usage statistics.</p>"},{"location":"api/usage/#constructor_1","title":"Constructor","text":"<pre><code>def __init__(self, config: Request):\n    \"\"\"Initialize the UsageLogger with a configuration.\"\"\"\n</code></pre>"},{"location":"api/usage/#parameters_1","title":"Parameters","text":"<ul> <li><code>config</code> (<code>Request</code>): The configuration for the usage logger.</li> </ul>"},{"location":"api/usage/#methods_1","title":"Methods","text":""},{"location":"api/usage/#get_usage_stats_1","title":"get_usage_stats","text":"<pre><code>def get_usage_stats(self) -&gt; dict:\n    \"\"\"Get usage statistics.\"\"\"\n</code></pre> <p>Gets usage statistics.</p> <p>Returns: - A dictionary with usage statistics.</p> <p>Example: <pre><code>usage_stats = logger.get_usage_stats()\n</code></pre></p>"},{"location":"api/usage/#log_request_1","title":"log_request","text":"<pre><code>def log_request(\n    self,\n    request_id: str,\n    model_name: str,\n    model_provider: str,\n    client_id: str,\n    user_id: str,\n    input_tokens: int,\n    output_tokens: int,\n    input_cost: float,\n    output_cost: float,\n    status: str,\n    error_message: str = None\n) -&gt; None:\n    \"\"\"Log a request.\"\"\"\n</code></pre> <p>Logs a request.</p> <p>Parameters: - <code>request_id</code> (<code>str</code>): The ID of the request. - <code>model_name</code> (<code>str</code>): The name of the model. - <code>model_provider</code> (<code>str</code>): The provider of the model. - <code>client_id</code> (<code>str</code>): The client ID. - <code>user_id</code> (<code>str</code>): The user ID. - <code>input_tokens</code> (<code>int</code>): The number of input tokens. - <code>output_tokens</code> (<code>int</code>): The number of output tokens. - <code>input_cost</code> (<code>float</code>): The cost of input tokens. - <code>output_cost</code> (<code>float</code>): The cost of output tokens. - <code>status</code> (<code>str</code>): The status of the request. - <code>error_message</code> (<code>str</code>, optional): The error message, if any.</p> <p>Example: <pre><code>logger.log_request(\n    request_id=\"123\",\n    model_name=\"gpt-4\",\n    model_provider=\"openai\",\n    client_id=\"my_app\",\n    user_id=\"user123\",\n    input_tokens=10,\n    output_tokens=20,\n    input_cost=0.0001,\n    output_cost=0.0002,\n    status=\"success\"\n)\n</code></pre></p>"},{"location":"api/usage/#usage-models","title":"Usage Models","text":""},{"location":"api/usage/#clientusagedata","title":"ClientUsageData","text":"<p>The <code>ClientUsageData</code> class represents detailed usage data for a client.</p> <pre><code>class ClientUsageData(BaseModel):\n    \"\"\"Detailed usage data for a client.\"\"\"\n    client_id: str\n    total_requests: int\n    successful_requests: int\n    failed_requests: int\n    total_input_tokens: int\n    total_output_tokens: int\n    total_cost: float\n    models_used: List[ModelUsage]\n    recent_requests: List[RequestSummary]\n</code></pre>"},{"location":"api/usage/#fields","title":"Fields","text":"<ul> <li><code>client_id</code> (<code>str</code>): The client ID.</li> <li><code>total_requests</code> (<code>int</code>): The total number of requests.</li> <li><code>successful_requests</code> (<code>int</code>): The number of successful requests.</li> <li><code>failed_requests</code> (<code>int</code>): The number of failed requests.</li> <li><code>total_input_tokens</code> (<code>int</code>): The total number of input tokens.</li> <li><code>total_output_tokens</code> (<code>int</code>): The total number of output tokens.</li> <li><code>total_cost</code> (<code>float</code>): The total cost in USD.</li> <li><code>models_used</code> (<code>List[ModelUsage]</code>): A list of models used.</li> <li><code>recent_requests</code> (<code>List[RequestSummary]</code>): A list of recent requests.</li> </ul>"},{"location":"api/usage/#modelusage","title":"ModelUsage","text":"<p>The <code>ModelUsage</code> class represents usage statistics for a specific model.</p> <pre><code>class ModelUsage(BaseModel):\n    \"\"\"Usage statistics for a specific model.\"\"\"\n    model_name: str\n    model_provider: str\n    total_input_tokens: int\n    total_output_tokens: int\n    total_cost: float\n    last_used: datetime\n</code></pre>"},{"location":"api/usage/#fields_1","title":"Fields","text":"<ul> <li><code>model_name</code> (<code>str</code>): The name of the model.</li> <li><code>model_provider</code> (<code>str</code>): The provider of the model.</li> <li><code>total_input_tokens</code> (<code>int</code>): The total number of input tokens.</li> <li><code>total_output_tokens</code> (<code>int</code>): The total number of output tokens.</li> <li><code>total_cost</code> (<code>float</code>): The total cost in USD.</li> <li><code>last_used</code> (<code>datetime</code>): The last time the model was used.</li> </ul>"},{"location":"api/usage/#requestsummary","title":"RequestSummary","text":"<p>The <code>RequestSummary</code> class represents a summary of a request.</p> <pre><code>class RequestSummary(BaseModel):\n    \"\"\"Summary of a request.\"\"\"\n    request_id: str\n    timestamp: datetime\n    model_name: str\n    model_provider: str\n    input_tokens: int\n    output_tokens: int\n    total_cost: float\n    status: str\n    error_message: Optional[str] = None\n</code></pre>"},{"location":"api/usage/#fields_2","title":"Fields","text":"<ul> <li><code>request_id</code> (<code>str</code>): The ID of the request.</li> <li><code>timestamp</code> (<code>datetime</code>): The timestamp of the request.</li> <li><code>model_name</code> (<code>str</code>): The name of the model.</li> <li><code>model_provider</code> (<code>str</code>): The provider of the model.</li> <li><code>input_tokens</code> (<code>int</code>): The number of input tokens.</li> <li><code>output_tokens</code> (<code>int</code>): The number of output tokens.</li> <li><code>total_cost</code> (<code>float</code>): The total cost in USD.</li> <li><code>status</code> (<code>str</code>): The status of the request.</li> <li><code>error_message</code> (<code>Optional[str]</code>): The error message, if any.</li> </ul>"},{"location":"api/usage/#requestdetails","title":"RequestDetails","text":"<p>The <code>RequestDetails</code> class represents detailed information about a specific request.</p> <pre><code>class RequestDetails(BaseModel):\n    \"\"\"Detailed information about a specific request.\"\"\"\n    request_id: str\n    timestamp: datetime\n    model_name: str\n    model_provider: str\n    client_id: str\n    user_id: str\n    config_json: str\n    request_raw: str\n    request_json: str\n    response_json: str\n    response_raw: str\n    input_tokens: int\n    output_tokens: int\n    input_cost: float\n    output_cost: float\n    total_cost: float\n    status: str\n    error_message: Optional[str] = None\n</code></pre>"},{"location":"api/usage/#fields_3","title":"Fields","text":"<ul> <li><code>request_id</code> (<code>str</code>): The request ID.</li> <li><code>timestamp</code> (<code>datetime</code>): The timestamp of the request.</li> <li><code>model_name</code> (<code>str</code>): The name of the model.</li> <li><code>model_provider</code> (<code>str</code>): The provider of the model.</li> <li><code>client_id</code> (<code>str</code>): The client ID.</li> <li><code>user_id</code> (<code>str</code>): The user ID.</li> <li><code>config_json</code> (<code>str</code>): The configuration as JSON.</li> <li><code>request_raw</code> (<code>str</code>): The raw request.</li> <li><code>request_json</code> (<code>str</code>): The request as JSON.</li> <li><code>response_json</code> (<code>str</code>): The response as JSON.</li> <li><code>response_raw</code> (<code>str</code>): The raw response.</li> <li><code>input_tokens</code> (<code>int</code>): The number of input tokens.</li> <li><code>output_tokens</code> (<code>int</code>): The number of output tokens.</li> <li><code>input_cost</code> (<code>float</code>): The cost of input tokens.</li> <li><code>output_cost</code> (<code>float</code>): The cost of output tokens.</li> <li><code>total_cost</code> (<code>float</code>): The total cost.</li> <li><code>status</code> (<code>str</code>): The status of the request.</li> <li><code>error_message</code> (<code>Optional[str]</code>): The error message, if any.</li> </ul>"},{"location":"examples/agent-system/","title":"Agent System Example","text":"<p>This example demonstrates how to use Pydantic2's agent system to create AI agents with specific capabilities.</p>"},{"location":"examples/agent-system/#basic-agent-example","title":"Basic Agent Example","text":"<pre><code>from pydantic2.agents import SimpleAgent, tool\nfrom pydantic import BaseModel, Field\nfrom typing import List\nimport datetime\n\nclass AgentResponse(BaseModel):\n    answer: str = Field(..., description=\"The main answer\")\n    reasoning: str = Field(..., description=\"The reasoning process\")\n    tools_used: List[str] = Field(default_factory=list)\n\n@tool(\"get_time\", \"Get the current time\")\ndef get_current_time() -&gt; str:\n    \"\"\"Get the current time.\"\"\"\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n@tool(\"process_text\", \"Convert text to uppercase\")\ndef process_text(text: str) -&gt; str:\n    \"\"\"Process text input.\"\"\"\n    return text.upper()\n\n# Create and configure the agent\nagent = SimpleAgent(model_id=\"openrouter/openai/gpt-4o-mini\")\nagent.add_tools([get_current_time, process_text])\n\n# Run the agent\nresult = agent.run(\"What time is it and convert 'hello world' to uppercase\")\nprint(result)\n</code></pre>"},{"location":"examples/agent-system/#advanced-agent-example","title":"Advanced Agent Example","text":"<p>Here's a more advanced example that demonstrates how to create an agent with multiple tools and a custom response model:</p> <pre><code>from pydantic2.agents import SimpleAgent, tool\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\nimport datetime\nimport requests\nimport json\n\nclass WeatherInfo(BaseModel):\n    temperature: float = Field(..., description=\"Temperature in Celsius\")\n    conditions: str = Field(..., description=\"Weather conditions\")\n    humidity: float = Field(..., description=\"Humidity percentage\")\n    wind_speed: float = Field(..., description=\"Wind speed in km/h\")\n\nclass ResearchResult(BaseModel):\n    topic: str = Field(..., description=\"The research topic\")\n    summary: str = Field(..., description=\"Summary of the research\")\n    key_points: List[str] = Field(..., description=\"Key points from the research\")\n    sources: List[str] = Field(default_factory=list, description=\"Sources used in the research\")\n\nclass AgentResponse(BaseModel):\n    answer: str = Field(..., description=\"The main answer to the user's query\")\n    reasoning: str = Field(..., description=\"The reasoning process\")\n    tools_used: List[str] = Field(default_factory=list, description=\"List of tools used\")\n    weather_info: Optional[WeatherInfo] = Field(None, description=\"Weather information if requested\")\n    research_results: Optional[ResearchResult] = Field(None, description=\"Research results if requested\")\n\n@tool(\"get_time\", \"Get the current time\")\ndef get_current_time() -&gt; str:\n    \"\"\"Get the current time.\"\"\"\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n@tool(\"get_weather\", \"Get the current weather for a location\")\ndef get_weather(location: str) -&gt; dict:\n    \"\"\"Get the current weather for a location.\"\"\"\n    # This is a mock implementation\n    # In a real application, you would use a weather API\n    weather_data = {\n        \"New York\": {\n            \"temperature\": 22.5,\n            \"conditions\": \"Partly cloudy\",\n            \"humidity\": 65.0,\n            \"wind_speed\": 10.2\n        },\n        \"London\": {\n            \"temperature\": 18.0,\n            \"conditions\": \"Rainy\",\n            \"humidity\": 80.0,\n            \"wind_speed\": 15.5\n        },\n        \"Tokyo\": {\n            \"temperature\": 28.0,\n            \"conditions\": \"Sunny\",\n            \"humidity\": 70.0,\n            \"wind_speed\": 8.0\n        }\n    }\n\n    return weather_data.get(location, {\n        \"temperature\": 20.0,\n        \"conditions\": \"Unknown\",\n        \"humidity\": 50.0,\n        \"wind_speed\": 5.0\n    })\n\n@tool(\"search_web\", \"Search the web for information\")\ndef search_web(query: str) -&gt; List[dict]:\n    \"\"\"Search the web for information.\"\"\"\n    # This is a mock implementation\n    # In a real application, you would use a search API\n    search_results = {\n        \"climate change\": [\n            {\"title\": \"Climate Change: Causes and Effects\", \"url\": \"https://example.com/climate-change\"},\n            {\"title\": \"Global Warming: The Science\", \"url\": \"https://example.com/global-warming\"},\n            {\"title\": \"Climate Action: What You Can Do\", \"url\": \"https://example.com/climate-action\"}\n        ],\n        \"artificial intelligence\": [\n            {\"title\": \"AI: An Introduction\", \"url\": \"https://example.com/ai-intro\"},\n            {\"title\": \"Machine Learning Basics\", \"url\": \"https://example.com/ml-basics\"},\n            {\"title\": \"The Future of AI\", \"url\": \"https://example.com/ai-future\"}\n        ]\n    }\n\n    # Return some default results if the query is not found\n    return search_results.get(query.lower(), [\n        {\"title\": f\"Result 1 for {query}\", \"url\": f\"https://example.com/result1-{query}\"},\n        {\"title\": f\"Result 2 for {query}\", \"url\": f\"https://example.com/result2-{query}\"},\n        {\"title\": f\"Result 3 for {query}\", \"url\": f\"https://example.com/result3-{query}\"}\n    ])\n\n@tool(\"calculate\", \"Perform a calculation\")\ndef calculate(expression: str) -&gt; float:\n    \"\"\"Perform a calculation.\"\"\"\n    try:\n        # Warning: eval can be dangerous in production code\n        # This is just for demonstration purposes\n        return eval(expression)\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\ndef main():\n    # Create and configure the agent\n    agent = SimpleAgent(\n        model_id=\"openrouter/openai/gpt-4o-mini\",\n        response_model=AgentResponse\n    )\n\n    # Add tools\n    agent.add_tools([\n        get_time,\n        get_weather,\n        search_web,\n        calculate\n    ])\n\n    # Run the agent with different queries\n    queries = [\n        \"What time is it now?\",\n        \"What's the weather like in Tokyo?\",\n        \"Research climate change and summarize the key points.\",\n        \"Calculate the square root of 144 and then add 10 to it.\"\n    ]\n\n    for query in queries:\n        print(f\"\\n\\nQuery: {query}\")\n        print(\"-\" * 50)\n\n        result = agent.run(query)\n\n        print(f\"Answer: {result.answer}\")\n        print(f\"Reasoning: {result.reasoning}\")\n        print(f\"Tools used: {', '.join(result.tools_used)}\")\n\n        if result.weather_info:\n            weather = result.weather_info\n            print(\"\\nWeather Information:\")\n            print(f\"Temperature: {weather.temperature}\u00b0C\")\n            print(f\"Conditions: {weather.conditions}\")\n            print(f\"Humidity: {weather.humidity}%\")\n            print(f\"Wind Speed: {weather.wind_speed} km/h\")\n\n        if result.research_results:\n            research = result.research_results\n            print(\"\\nResearch Results:\")\n            print(f\"Topic: {research.topic}\")\n            print(f\"Summary: {research.summary}\")\n            print(\"\\nKey Points:\")\n            for point in research.key_points:\n                print(f\"- {point}\")\n            if research.sources:\n                print(\"\\nSources:\")\n                for source in research.sources:\n                    print(f\"- {source}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/agent-system/#gradio-ui-example","title":"Gradio UI Example","text":"<p>You can also launch a Gradio UI for your agent:</p> <pre><code>from pydantic2.agents import SimpleAgent, tool\nfrom pydantic import BaseModel, Field\nfrom typing import List\nimport datetime\nimport random\n\nclass WeatherInfo(BaseModel):\n    temperature: float = Field(..., description=\"Temperature in Celsius\")\n    conditions: str = Field(..., description=\"Weather conditions\")\n    humidity: float = Field(..., description=\"Humidity percentage\")\n    wind_speed: float = Field(..., description=\"Wind speed in km/h\")\n\nclass AgentResponse(BaseModel):\n    answer: str = Field(..., description=\"The main answer\")\n    reasoning: str = Field(..., description=\"The reasoning process\")\n    tools_used: List[str] = Field(default_factory=list)\n    weather_info: Optional[WeatherInfo] = Field(None, description=\"Weather information if requested\")\n\n@tool(\"get_time\", \"Get the current time\")\ndef get_current_time() -&gt; str:\n    \"\"\"Get the current time.\"\"\"\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n@tool(\"get_weather\", \"Get the current weather for a location\")\ndef get_weather(location: str) -&gt; dict:\n    \"\"\"Get the current weather for a location.\"\"\"\n    # This is a mock implementation\n    weather_data = {\n        \"New York\": {\n            \"temperature\": 22.5,\n            \"conditions\": \"Partly cloudy\",\n            \"humidity\": 65.0,\n            \"wind_speed\": 10.2\n        },\n        \"London\": {\n            \"temperature\": 18.0,\n            \"conditions\": \"Rainy\",\n            \"humidity\": 80.0,\n            \"wind_speed\": 15.5\n        },\n        \"Tokyo\": {\n            \"temperature\": 28.0,\n            \"conditions\": \"Sunny\",\n            \"humidity\": 70.0,\n            \"wind_speed\": 8.0\n        }\n    }\n\n    return weather_data.get(location, {\n        \"temperature\": 20.0,\n        \"conditions\": \"Unknown\",\n        \"humidity\": 50.0,\n        \"wind_speed\": 5.0\n    })\n\n@tool(\"roll_dice\", \"Roll a dice with the specified number of sides\")\ndef roll_dice(sides: int = 6) -&gt; int:\n    \"\"\"Roll a dice with the specified number of sides.\"\"\"\n    return random.randint(1, sides)\n\n# Create and configure the agent\nagent = SimpleAgent(\n    model_id=\"openrouter/openai/gpt-4o-mini\",\n    response_model=AgentResponse\n)\n\n# Add tools\nagent.add_tools([\n    get_time,\n    get_weather,\n    roll_dice\n])\n\n# Launch Gradio UI\nagent.launch_gradio_ui(\n    title=\"Pydantic2 Agent Demo\",\n    description=\"Ask questions and the agent will answer using its tools.\",\n    theme=\"default\",\n    share=True\n)\n</code></pre>"},{"location":"examples/agent-system/#creating-custom-tools","title":"Creating Custom Tools","text":"<p>You can create custom tools for your agent:</p> <pre><code>from pydantic2.agents import SimpleAgent, tool\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\nimport requests\nimport json\n\nclass StockInfo(BaseModel):\n    symbol: str = Field(..., description=\"Stock symbol\")\n    price: float = Field(..., description=\"Current price\")\n    change: float = Field(..., description=\"Price change\")\n    change_percent: float = Field(..., description=\"Price change percentage\")\n\nclass AgentResponse(BaseModel):\n    answer: str = Field(..., description=\"The main answer\")\n    reasoning: str = Field(..., description=\"The reasoning process\")\n    tools_used: List[str] = Field(default_factory=list)\n    stock_info: Optional[StockInfo] = Field(None, description=\"Stock information if requested\")\n\n@tool(\"get_stock_price\", \"Get the current stock price\")\ndef get_stock_price(symbol: str) -&gt; dict:\n    \"\"\"Get the current stock price for a symbol.\"\"\"\n    # This is a mock implementation\n    # In a real application, you would use a stock API\n    stock_data = {\n        \"AAPL\": {\n            \"symbol\": \"AAPL\",\n            \"price\": 150.25,\n            \"change\": 2.75,\n            \"change_percent\": 1.86\n        },\n        \"MSFT\": {\n            \"symbol\": \"MSFT\",\n            \"price\": 290.50,\n            \"change\": -1.25,\n            \"change_percent\": -0.43\n        },\n        \"GOOGL\": {\n            \"symbol\": \"GOOGL\",\n            \"price\": 2750.75,\n            \"change\": 15.50,\n            \"change_percent\": 0.57\n        }\n    }\n\n    return stock_data.get(symbol.upper(), {\n        \"symbol\": symbol.upper(),\n        \"price\": 100.00,\n        \"change\": 0.00,\n        \"change_percent\": 0.00\n    })\n\n# Create and configure the agent\nagent = SimpleAgent(\n    model_id=\"openrouter/openai/gpt-4o-mini\",\n    response_model=AgentResponse\n)\n\n# Add tools\nagent.add_tools([get_stock_price])\n\n# Run the agent\nresult = agent.run(\"What's the current price of AAPL stock?\")\nprint(result)\n</code></pre>"},{"location":"examples/agent-system/#key-features","title":"Key Features","text":"<p>The agent system provides several key features:</p> <ol> <li>Tool Decorator: The <code>@tool</code> decorator makes it easy to define tools.</li> <li>Type Safety: Tools and responses are type-safe with Pydantic models.</li> <li>Gradio UI: You can launch a Gradio UI for your agent.</li> <li>Custom Response Models: You can define custom response models for your agent.</li> <li>Multiple Tools: You can add multiple tools to your agent.</li> </ol>"},{"location":"examples/agent-system/#next-steps","title":"Next Steps","text":"<p>Now that you've seen how to use Pydantic2's agent system, check out the API Reference to learn more about the available APIs.</p>"},{"location":"examples/basic-usage/","title":"Basic Usage Example","text":"<p>This example demonstrates how to use Pydantic2 for structured responses from LLMs.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List\nimport json\nimport uuid\n\nfrom pydantic2 import LiteLLMClient, Request\n\n\n# Define a custom response model\nclass UserDetail(BaseModel):\n    \"\"\"Model for extracting user details from text.\"\"\"\n    name: str = Field(description=\"The user's name\")\n    age: int = Field(description=\"The user's age\")\n    interests: List[str] = Field(description=\"List of user's interests\")\n\n\ndef main():\n    \"\"\"Example using LiteLLM client with OpenAI and cost tracking.\"\"\"\n    # Generate a unique user_id for the example\n    user_id = str(uuid.uuid4())\n\n    # Create a request configuration\n    config = Request(\n        model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",\n        temperature=0.7,\n        max_tokens=500,\n        max_budget=0.0001,  # $0.0001 USD (very small budget)\n        client_id='demo',\n        user_id=user_id,\n        answer_model=UserDetail,\n        verbose=False,\n        logs=False,\n        online=True,\n    )\n\n    # Initialize the client\n    client = LiteLLMClient(config)\n\n    client.msg.add_message_user(\"Describe who is David Copperfield\")\n\n    try:\n        # Generate a response\n        response: UserDetail = client.generate_response()\n\n        # Print the structured response\n        print(json.dumps(response.model_dump(), indent=2))\n\n    except Exception as e:\n        print(f\"Error: {e}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/basic-usage/#step-by-step-explanation","title":"Step-by-Step Explanation","text":""},{"location":"examples/basic-usage/#1-define-a-response-model","title":"1. Define a Response Model","text":"<p>First, we define a Pydantic model that represents the structure of the response we want:</p> <pre><code>class UserDetail(BaseModel):\n    \"\"\"Model for extracting user details from text.\"\"\"\n    name: str = Field(description=\"The user's name\")\n    age: int = Field(description=\"The user's age\")\n    interests: List[str] = Field(description=\"List of user's interests\")\n</code></pre>"},{"location":"examples/basic-usage/#2-configure-the-request","title":"2. Configure the Request","text":"<p>Next, we create a <code>Request</code> object with our desired configuration:</p> <pre><code>config = Request(\n    model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",\n    temperature=0.7,\n    max_tokens=500,\n    max_budget=0.0001,  # $0.0001 USD (very small budget)\n    client_id='demo',\n    user_id=user_id,\n    answer_model=UserDetail,\n    verbose=False,\n    logs=False,\n    online=True,\n)\n</code></pre>"},{"location":"examples/basic-usage/#3-initialize-the-client","title":"3. Initialize the Client","text":"<p>We create a <code>LiteLLMClient</code> with our configuration:</p> <pre><code>client = LiteLLMClient(config)\n</code></pre>"},{"location":"examples/basic-usage/#4-add-a-message","title":"4. Add a Message","text":"<p>We add a message to the conversation:</p> <pre><code>client.msg.add_message_user(\"Describe who is David Copperfield\")\n</code></pre>"},{"location":"examples/basic-usage/#5-generate-a-response","title":"5. Generate a Response","text":"<p>We generate a response and get it in our structured format:</p> <pre><code>response: UserDetail = client.generate_response()\n</code></pre>"},{"location":"examples/basic-usage/#6-access-the-response","title":"6. Access the Response","text":"<p>We can access the fields of the response directly or convert it to JSON:</p> <pre><code>print(json.dumps(response.model_dump(), indent=2))\n</code></pre>"},{"location":"examples/basic-usage/#expected-output","title":"Expected Output","text":"<p>The output of this example will look something like this:</p> <pre><code>Using user_id: 123e4567-e89b-12d3-a456-426614174000\nSetting budget: $0.000100 USD (very small budget)\n\nPrompt token count: 12\nMax tokens for model: 4096\n\n=== Budget Information ===\nmax_budget: $0.000100\nestimated_cost: $0.000012\nremaining_budget: $0.000088\nbudget_exceeded: False\n\nStructured Response:\n{\n  \"name\": \"David Copperfield\",\n  \"age\": 67,\n  \"interests\": [\n    \"Magic\",\n    \"Illusions\",\n    \"Performance art\",\n    \"Collecting rare artifacts\",\n    \"Philanthropy\"\n  ]\n}\n\nModel used: openrouter/openai/gpt-4o-mini-2024-07-18\nResponse time: 1.234 seconds\nTotal token count: 56\n\n=== Usage Information ===\nModel: openrouter/openai/gpt-4o-mini-2024-07-18\nInput tokens: 12\nOutput tokens: 44\nTotal tokens: 56\nCost: $0.000056\n</code></pre>"},{"location":"examples/basic-usage/#next-steps","title":"Next Steps","text":"<p>Now that you've seen a basic example, check out the Django Integration example to learn how to integrate Pydantic2 with Django.</p>"},{"location":"examples/django-integration/","title":"Django Integration Example","text":"<p>This example demonstrates how to integrate Pydantic2 with Django REST framework.</p> <pre><code>from rest_framework.views import APIView\nfrom rest_framework.response import Response\nfrom rest_framework import serializers\nfrom pydantic import BaseModel, Field\nfrom typing import List\nfrom pydantic2 import Request, LiteLLMClient\n\nclass FeedbackAnalysis(BaseModel):\n    summary: str = Field(..., description=\"Summary of the feedback\")\n    sentiment: str = Field(..., description=\"Detected sentiment\")\n    key_points: List[str] = Field(..., description=\"Key points from the feedback\")\n\nclass FeedbackResponseSerializer(serializers.Serializer):\n    answer = FeedbackAnalysis.drf_serializer()\n\nclass FeedbackView(APIView):\n    def post(self, request):\n        feedback = request.data.get('feedback', '')\n\n        client = LiteLLMClient(Request(\n            model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",\n            temperature=0.3,\n            answer_model=FeedbackAnalysis,\n            max_budget=0.01,\n            user_id=request.user.id if hasattr(request, 'user') else None,\n            client_id=\"django_feedback_app\"\n        ))\n\n        client.msg.add_message_system(\"You are a feedback analysis expert.\")\n        client.msg.add_message_user(feedback)\n\n        response: FeedbackAnalysis = client.generate_response()\n\n        serializer = FeedbackResponseSerializer(data={\n            \"answer\": response.model_dump()\n        })\n        serializer.is_valid(raise_exception=True)\n\n        return Response(serializer.data)\n</code></pre> <p>Key features: - Seamless integration with Django REST framework - Automatic serialization of Pydantic models - Type-safe response handling - Built-in validation - User tracking through Django's authentication system</p>"},{"location":"examples/django-integration/#complete-example","title":"Complete Example","text":"<pre><code>from rest_framework.views import APIView\nfrom rest_framework.response import Response\nfrom rest_framework import serializers\nfrom pydantic import BaseModel, Field\nfrom typing import List\nfrom pydantic2 import Request, LiteLLMClient\n\nclass FeedbackAnalysis(BaseModel):\n    summary: str = Field(..., description=\"Summary of the feedback\")\n    sentiment: str = Field(..., description=\"Detected sentiment\")\n    key_points: List[str] = Field(..., description=\"Key points from the feedback\")\n\nclass FeedbackResponseSerializer(serializers.Serializer):\n    answer = serializers.JSONField()\n\nclass FeedbackView(APIView):\n    def post(self, request):\n        feedback = request.data.get('feedback', '')\n\n        client = LiteLLMClient(Request(\n            model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",\n            temperature=0.3,\n            answer_model=FeedbackAnalysis,\n            max_budget=0.01,\n            user_id=request.user.id if hasattr(request, 'user') else None,\n            client_id=\"django_feedback_app\"\n        ))\n\n        client.msg.add_message_system(\"You are a feedback analysis expert.\")\n        client.msg.add_message_user(feedback)\n\n        response: FeedbackAnalysis = client.generate_response()\n\n        serializer = FeedbackResponseSerializer(data={\n            \"answer\": response.model_dump()\n        })\n        serializer.is_valid(raise_exception=True)\n\n        return Response(serializer.data)\n</code></pre>"},{"location":"examples/django-integration/#step-by-step-explanation","title":"Step-by-Step Explanation","text":""},{"location":"examples/django-integration/#1-define-a-response-model","title":"1. Define a Response Model","text":"<p>First, we define a Pydantic model that represents the structure of the response we want:</p> <pre><code>class FeedbackAnalysis(BaseModel):\n    summary: str = Field(..., description=\"Summary of the feedback\")\n    sentiment: str = Field(..., description=\"Detected sentiment\")\n    key_points: List[str] = Field(..., description=\"Key points from the feedback\")\n</code></pre>"},{"location":"examples/django-integration/#2-define-a-serializer","title":"2. Define a Serializer","text":"<p>Next, we define a Django REST framework serializer to handle the response:</p> <pre><code>class FeedbackResponseSerializer(serializers.Serializer):\n    answer = serializers.JSONField()\n</code></pre>"},{"location":"examples/django-integration/#3-create-an-api-view","title":"3. Create an API View","text":"<p>We create a Django REST framework API view to handle the request:</p> <pre><code>class FeedbackView(APIView):\n    def post(self, request):\n        feedback = request.data.get('feedback', '')\n\n        client = LiteLLMClient(Request(\n            model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",\n            temperature=0.3,\n            answer_model=FeedbackAnalysis,\n            max_budget=0.01,\n            user_id=request.user.id if hasattr(request, 'user') else None,\n            client_id=\"django_feedback_app\"\n        ))\n\n        client.msg.add_message_system(\"You are a feedback analysis expert.\")\n        client.msg.add_message_user(feedback)\n\n        response: FeedbackAnalysis = client.generate_response()\n\n        serializer = FeedbackResponseSerializer(data={\n            \"answer\": response.model_dump()\n        })\n        serializer.is_valid(raise_exception=True)\n\n        return Response(serializer.data)\n</code></pre>"},{"location":"examples/django-integration/#complete-django-project-example","title":"Complete Django Project Example","text":"<p>Here's a more complete example of a Django project that uses Pydantic2:</p>"},{"location":"examples/django-integration/#modelspy","title":"models.py","text":"<pre><code>from django.db import models\nfrom django.contrib.auth.models import User\n\nclass Feedback(models.Model):\n    user = models.ForeignKey(User, on_delete=models.CASCADE, null=True, blank=True)\n    text = models.TextField()\n    created_at = models.DateTimeField(auto_now_add=True)\n\n    def __str__(self):\n        return f\"Feedback from {self.user or 'Anonymous'} at {self.created_at}\"\n\nclass FeedbackAnalysisResult(models.Model):\n    feedback = models.OneToOneField(Feedback, on_delete=models.CASCADE, related_name='analysis')\n    summary = models.TextField()\n    sentiment = models.CharField(max_length=50)\n    key_points = models.JSONField()\n    created_at = models.DateTimeField(auto_now_add=True)\n\n    def __str__(self):\n        return f\"Analysis for {self.feedback}\"\n</code></pre>"},{"location":"examples/django-integration/#serializerspy","title":"serializers.py","text":"<pre><code>from rest_framework import serializers\nfrom .models import Feedback, FeedbackAnalysisResult\n\nclass FeedbackSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = Feedback\n        fields = ['id', 'text', 'created_at']\n        read_only_fields = ['created_at']\n\nclass FeedbackAnalysisResultSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = FeedbackAnalysisResult\n        fields = ['id', 'summary', 'sentiment', 'key_points', 'created_at']\n        read_only_fields = ['created_at']\n\nclass FeedbackWithAnalysisSerializer(serializers.ModelSerializer):\n    analysis = FeedbackAnalysisResultSerializer(read_only=True)\n\n    class Meta:\n        model = Feedback\n        fields = ['id', 'text', 'created_at', 'analysis']\n        read_only_fields = ['created_at']\n</code></pre>"},{"location":"examples/django-integration/#viewspy","title":"views.py","text":"<pre><code>from rest_framework.views import APIView\nfrom rest_framework.response import Response\nfrom rest_framework import status, permissions\nfrom pydantic import BaseModel, Field\nfrom typing import List\nfrom pydantic2 import Request, LiteLLMClient\n\nfrom .models import Feedback, FeedbackAnalysisResult\nfrom .serializers import FeedbackSerializer, FeedbackWithAnalysisSerializer\n\nclass FeedbackAnalysis(BaseModel):\n    summary: str = Field(..., description=\"Summary of the feedback\")\n    sentiment: str = Field(..., description=\"Detected sentiment\")\n    key_points: List[str] = Field(..., description=\"Key points from the feedback\")\n\nclass FeedbackView(APIView):\n    permission_classes = [permissions.IsAuthenticated]\n\n    def get(self, request):\n        feedbacks = Feedback.objects.filter(user=request.user)\n        serializer = FeedbackWithAnalysisSerializer(feedbacks, many=True)\n        return Response(serializer.data)\n\n    def post(self, request):\n        serializer = FeedbackSerializer(data=request.data)\n        if serializer.is_valid():\n            feedback = serializer.save(user=request.user)\n\n            # Analyze the feedback using Pydantic2\n            client = LiteLLMClient(Request(\n                model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",\n                temperature=0.3,\n                answer_model=FeedbackAnalysis,\n                max_budget=0.01,\n                user_id=str(request.user.id),\n                client_id=\"django_feedback_app\"\n            ))\n\n            client.msg.add_message_system(\"You are a feedback analysis expert.\")\n            client.msg.add_message_user(feedback.text)\n\n            response: FeedbackAnalysis = client.generate_response()\n\n            # Save the analysis result\n            analysis = FeedbackAnalysisResult.objects.create(\n                feedback=feedback,\n                summary=response.summary,\n                sentiment=response.sentiment,\n                key_points=response.key_points\n            )\n\n            # Return the feedback with analysis\n            result_serializer = FeedbackWithAnalysisSerializer(feedback)\n            return Response(result_serializer.data, status=status.HTTP_201_CREATED)\n\n        return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)\n</code></pre>"},{"location":"examples/django-integration/#urlspy","title":"urls.py","text":"<pre><code>from django.urls import path\nfrom .views import FeedbackView\n\nurlpatterns = [\n    path('feedback/', FeedbackView.as_view(), name='feedback'),\n]\n</code></pre>"},{"location":"examples/django-integration/#key-features","title":"Key Features","text":"<p>The Django integration example demonstrates several key features:</p> <ol> <li>Seamless integration with Django REST framework: Pydantic2 works well with Django's serialization system.</li> <li>User tracking: The example uses Django's authentication system to track users.</li> <li>Database integration: The example saves the analysis results to a database.</li> <li>Structured responses: The example uses Pydantic models to define the structure of the response.</li> <li>Budget management: The example sets a budget limit for each request.</li> </ol>"},{"location":"examples/django-integration/#next-steps","title":"Next Steps","text":"<p>Now that you've seen how to integrate Pydantic2 with Django, check out the FastAPI Integration example to learn how to integrate Pydantic2 with FastAPI.</p>"},{"location":"examples/fastapi-integration/","title":"FastAPI Integration Example","text":"<p>This example demonstrates how to integrate Pydantic2 with FastAPI to create a text analysis API.</p>"},{"location":"examples/fastapi-integration/#complete-example","title":"Complete Example","text":"<pre><code>from fastapi import FastAPI, HTTPException, Depends, Query\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\nimport uuid\n\nfrom pydantic2 import LiteLLMClient, Request\n\n# Define a response model\nclass TextAnalysis(BaseModel):\n    \"\"\"Model for text analysis results.\"\"\"\n    summary: str = Field(description=\"Summary of the text\")\n    topics: List[str] = Field(description=\"Main topics in the text\")\n    sentiment: str = Field(description=\"Overall sentiment of the text\")\n    key_phrases: List[str] = Field(description=\"Key phrases extracted from the text\")\n    reading_time: int = Field(description=\"Estimated reading time in minutes\")\n    language: str = Field(description=\"Detected language of the text\")\n\n# Create a FastAPI app\napp = FastAPI(\n    title=\"Text Analysis API\",\n    description=\"API for analyzing text using Pydantic2\",\n    version=\"1.0.0\"\n)\n\n# Create a shared client for reuse\ndef get_client(user_id: str = None):\n    \"\"\"Get a LiteLLMClient instance.\"\"\"\n    config = Request(\n        model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",\n        temperature=0.3,\n        max_tokens=500,\n        answer_model=TextAnalysis,\n        max_budget=0.01,\n        user_id=user_id,\n        client_id=\"fastapi_text_analysis\",\n        verbose=False\n    )\n    return LiteLLMClient(config)\n\n@app.post(\"/analyze\", response_model=TextAnalysis)\nasync def analyze_text(\n    text: str,\n    user_id: Optional[str] = Query(None, description=\"User ID for tracking\"),\n):\n    \"\"\"Analyze text and return structured results.\"\"\"\n    # Generate a user ID if not provided\n    if not user_id:\n        user_id = str(uuid.uuid4())\n\n    # Get a client\n    client = get_client(user_id)\n\n    # Add messages\n    client.msg.add_message_system(\"You are a text analysis expert. Analyze the following text.\")\n    client.msg.add_message_user(text)\n\n    try:\n        # Generate a response\n        response: TextAnalysis = client.generate_response()\n        return response\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/usage/{user_id}\")\nasync def get_usage(user_id: str):\n    \"\"\"Get usage statistics for a user.\"\"\"\n    client = get_client(user_id)\n    try:\n        usage_data = client.usage_tracker.get_client_usage_data(client_id=\"fastapi_text_analysis\")\n        return {\n            \"user_id\": user_id,\n            \"total_requests\": usage_data.total_requests,\n            \"successful_requests\": usage_data.successful_requests,\n            \"failed_requests\": usage_data.failed_requests,\n            \"total_cost\": usage_data.total_cost,\n            \"models_used\": [model.model_name for model in usage_data.models_used]\n        }\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/request/{request_id}\")\nasync def get_request_details(request_id: str):\n    \"\"\"Get details for a specific request.\"\"\"\n    client = get_client()\n    try:\n        request_details = client.usage_tracker.get_request_details(request_id=request_id)\n        if not request_details:\n            raise HTTPException(status_code=404, detail=\"Request not found\")\n        return request_details\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n</code></pre>"},{"location":"examples/fastapi-integration/#step-by-step-explanation","title":"Step-by-Step Explanation","text":""},{"location":"examples/fastapi-integration/#1-define-a-response-model","title":"1. Define a Response Model","text":"<p>First, we define a Pydantic model that represents the structure of the response we want:</p> <pre><code>class TextAnalysis(BaseModel):\n    \"\"\"Model for text analysis results.\"\"\"\n    summary: str = Field(description=\"Summary of the text\")\n    topics: List[str] = Field(description=\"Main topics in the text\")\n    sentiment: str = Field(description=\"Overall sentiment of the text\")\n    key_phrases: List[str] = Field(description=\"Key phrases extracted from the text\")\n    reading_time: int = Field(description=\"Estimated reading time in minutes\")\n    language: str = Field(description=\"Detected language of the text\")\n</code></pre>"},{"location":"examples/fastapi-integration/#2-create-a-fastapi-app","title":"2. Create a FastAPI App","text":"<p>Next, we create a FastAPI app:</p> <pre><code>app = FastAPI(\n    title=\"Text Analysis API\",\n    description=\"API for analyzing text using Pydantic2\",\n    version=\"1.0.0\"\n)\n</code></pre>"},{"location":"examples/fastapi-integration/#3-create-a-client-factory","title":"3. Create a Client Factory","text":"<p>We create a function to get a client instance:</p> <pre><code>def get_client(user_id: str = None):\n    \"\"\"Get a LiteLLMClient instance.\"\"\"\n    config = Request(\n        model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",\n        temperature=0.3,\n        max_tokens=500,\n        answer_model=TextAnalysis,\n        max_budget=0.01,\n        user_id=user_id,\n        client_id=\"fastapi_text_analysis\",\n        verbose=False\n    )\n    return LiteLLMClient(config)\n</code></pre>"},{"location":"examples/fastapi-integration/#4-create-api-endpoints","title":"4. Create API Endpoints","text":"<p>We create API endpoints for analyzing text and getting usage statistics:</p> <pre><code>@app.post(\"/analyze\", response_model=TextAnalysis)\nasync def analyze_text(\n    text: str,\n    user_id: Optional[str] = Query(None, description=\"User ID for tracking\"),\n):\n    \"\"\"Analyze text and return structured results.\"\"\"\n    # Generate a user ID if not provided\n    if not user_id:\n        user_id = str(uuid.uuid4())\n\n    # Get a client\n    client = get_client(user_id)\n\n    # Add messages\n    client.msg.add_message_system(\"You are a text analysis expert. Analyze the following text.\")\n    client.msg.add_message_user(text)\n\n    try:\n        # Generate a response\n        response: TextAnalysis = client.generate_response()\n        return response\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/usage/{user_id}\")\nasync def get_usage(user_id: str):\n    \"\"\"Get usage statistics for a user.\"\"\"\n    client = get_client(user_id)\n    try:\n        usage_data = client.usage_tracker.get_client_usage_data(client_id=\"fastapi_text_analysis\")\n        return {\n            \"user_id\": user_id,\n            \"total_requests\": usage_data.total_requests,\n            \"successful_requests\": usage_data.successful_requests,\n            \"failed_requests\": usage_data.failed_requests,\n            \"total_cost\": usage_data.total_cost,\n            \"models_used\": [model.model_name for model in usage_data.models_used]\n        }\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/request/{request_id}\")\nasync def get_request_details(request_id: str):\n    \"\"\"Get details for a specific request.\"\"\"\n    client = get_client()\n    try:\n        request_details = client.usage_tracker.get_request_details(request_id=request_id)\n        if not request_details:\n            raise HTTPException(status_code=404, detail=\"Request not found\")\n        return request_details\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n</code></pre>"},{"location":"examples/fastapi-integration/#complete-fastapi-project-example","title":"Complete FastAPI Project Example","text":"<p>Here's a more complete example of a FastAPI project that uses Pydantic2:</p>"},{"location":"examples/fastapi-integration/#modelspy","title":"models.py","text":"<pre><code>from pydantic import BaseModel, Field\nfrom typing import List, Optional\nfrom datetime import datetime\n\nclass TextAnalysisRequest(BaseModel):\n    \"\"\"Request model for text analysis.\"\"\"\n    text: str = Field(..., description=\"Text to analyze\")\n    user_id: Optional[str] = Field(None, description=\"User ID for tracking\")\n\nclass TextAnalysis(BaseModel):\n    \"\"\"Model for text analysis results.\"\"\"\n    summary: str = Field(description=\"Summary of the text\")\n    topics: List[str] = Field(description=\"Main topics in the text\")\n    sentiment: str = Field(description=\"Overall sentiment of the text\")\n    key_phrases: List[str] = Field(description=\"Key phrases extracted from the text\")\n    reading_time: int = Field(description=\"Estimated reading time in minutes\")\n    language: str = Field(description=\"Detected language of the text\")\n\nclass TextAnalysisResponse(BaseModel):\n    \"\"\"Response model for text analysis.\"\"\"\n    request_id: str = Field(..., description=\"Request ID\")\n    timestamp: datetime = Field(..., description=\"Timestamp of the analysis\")\n    analysis: TextAnalysis = Field(..., description=\"Analysis results\")\n    model_used: str = Field(..., description=\"Model used for analysis\")\n    processing_time: float = Field(..., description=\"Processing time in seconds\")\n\nclass UsageResponse(BaseModel):\n    \"\"\"Response model for usage statistics.\"\"\"\n    user_id: str = Field(..., description=\"User ID\")\n    total_requests: int = Field(..., description=\"Total number of requests\")\n    successful_requests: int = Field(..., description=\"Number of successful requests\")\n    failed_requests: int = Field(..., description=\"Number of failed requests\")\n    total_cost: float = Field(..., description=\"Total cost in USD\")\n    models_used: List[str] = Field(..., description=\"Models used\")\n</code></pre>"},{"location":"examples/fastapi-integration/#dependenciespy","title":"dependencies.py","text":"<pre><code>from fastapi import Depends, HTTPException, status\nfrom fastapi.security import APIKeyHeader\nimport os\nfrom typing import Optional\n\nfrom pydantic2 import LiteLLMClient, Request\nfrom .models import TextAnalysis\n\n# API key security\nAPI_KEY_NAME = \"X-API-Key\"\nAPI_KEY = os.getenv(\"API_KEY\", \"test-api-key\")  # Default for development\n\napi_key_header = APIKeyHeader(name=API_KEY_NAME)\n\ndef verify_api_key(api_key: str = Depends(api_key_header)):\n    \"\"\"Verify the API key.\"\"\"\n    if api_key != API_KEY:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid API key\",\n            headers={\"WWW-Authenticate\": API_KEY_NAME},\n        )\n    return api_key\n\ndef get_client(user_id: Optional[str] = None):\n    \"\"\"Get a LiteLLMClient instance.\"\"\"\n    config = Request(\n        model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",\n        temperature=0.3,\n        max_tokens=500,\n        answer_model=TextAnalysis,\n        max_budget=0.01,\n        user_id=user_id,\n        client_id=\"fastapi_text_analysis\",\n        verbose=False\n    )\n    return LiteLLMClient(config)\n</code></pre>"},{"location":"examples/fastapi-integration/#mainpy","title":"main.py","text":"<pre><code>from fastapi import FastAPI, HTTPException, Depends, Query\nfrom typing import Optional\nimport uuid\nfrom datetime import datetime\n\nfrom .models import TextAnalysisRequest, TextAnalysisResponse, UsageResponse\nfrom .dependencies import get_client, verify_api_key\n\n# Create a FastAPI app\napp = FastAPI(\n    title=\"Text Analysis API\",\n    description=\"API for analyzing text using Pydantic2\",\n    version=\"1.0.0\"\n)\n\n@app.post(\"/analyze\", response_model=TextAnalysisResponse, dependencies=[Depends(verify_api_key)])\nasync def analyze_text(request: TextAnalysisRequest):\n    \"\"\"Analyze text and return structured results.\"\"\"\n    # Generate a user ID if not provided\n    user_id = request.user_id or str(uuid.uuid4())\n\n    # Get a client\n    client = get_client(user_id)\n\n    # Add messages\n    client.msg.add_message_system(\"You are a text analysis expert. Analyze the following text.\")\n    client.msg.add_message_user(request.text)\n\n    try:\n        # Record start time\n        start_time = datetime.now()\n\n        # Generate a response\n        analysis = client.generate_response()\n\n        # Calculate processing time\n        processing_time = (datetime.now() - start_time).total_seconds()\n\n        # Create response\n        response = TextAnalysisResponse(\n            request_id=client.meta.request_id,\n            timestamp=datetime.now(),\n            analysis=analysis,\n            model_used=client.meta.model_used,\n            processing_time=processing_time\n        )\n\n        return response\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/usage/{user_id}\", response_model=UsageResponse, dependencies=[Depends(verify_api_key)])\nasync def get_usage(user_id: str):\n    \"\"\"Get usage statistics for a user.\"\"\"\n    client = get_client(user_id)\n    try:\n        usage_data = client.usage_tracker.get_client_usage_data(client_id=\"fastapi_text_analysis\")\n        return UsageResponse(\n            user_id=user_id,\n            total_requests=usage_data.total_requests,\n            successful_requests=usage_data.successful_requests,\n            failed_requests=usage_data.failed_requests,\n            total_cost=usage_data.total_cost,\n            models_used=[model.model_name for model in usage_data.models_used]\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/request/{request_id}\", dependencies=[Depends(verify_api_key)])\nasync def get_request_details(request_id: str):\n    \"\"\"Get details for a specific request.\"\"\"\n    client = get_client()\n    try:\n        request_details = client.usage_tracker.get_request_details(request_id=request_id)\n        if not request_details:\n            raise HTTPException(status_code=404, detail=\"Request not found\")\n        return request_details\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n</code></pre>"},{"location":"examples/fastapi-integration/#key-features","title":"Key Features","text":"<p>The FastAPI integration example demonstrates several key features:</p> <ol> <li>Seamless integration with FastAPI: Pydantic2 works well with FastAPI's type system.</li> <li>User tracking: The example uses user IDs to track usage.</li> <li>API key authentication: The example uses API keys for authentication.</li> <li>Structured responses: The example uses Pydantic models to define the structure of the response.</li> <li>Budget management: The example sets a budget limit for each request.</li> <li>Usage tracking: The example provides endpoints for tracking usage.</li> </ol>"},{"location":"examples/fastapi-integration/#next-steps","title":"Next Steps","text":"<p>Now that you've seen how to integrate Pydantic2 with FastAPI, check out the Agent System example to learn how to use Pydantic2's agent system.</p>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>Pydantic2 provides a flexible configuration system through the <code>Request</code> class. This guide covers all the available configuration options and how to use them.</p>"},{"location":"getting-started/configuration/#basic-configuration","title":"Basic Configuration","text":"<p>Here's a basic example of configuring a request:</p> <pre><code>from pydantic2 import Request\nfrom your_app.models import YourResponseModel\n\nconfig = Request(\n    model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",\n    answer_model=YourResponseModel,\n    temperature=0.7,\n    max_tokens=500\n)\n</code></pre>"},{"location":"getting-started/configuration/#all-configuration-options","title":"All Configuration Options","text":"<p>The <code>Request</code> class accepts the following parameters:</p>"},{"location":"getting-started/configuration/#model-settings","title":"Model Settings","text":"Parameter Type Default Description <code>model</code> <code>str</code> <code>\"openrouter/openai/gpt-4o-mini-2024-07-18\"</code> Model identifier <code>answer_model</code> <code>Type[BaseModel]</code> Required Pydantic model for responses <code>temperature</code> <code>float</code> <code>0.7</code> Response randomness (0.0-1.0) <code>max_tokens</code> <code>int</code> <code>500</code> Maximum response length <code>top_p</code> <code>float</code> <code>1.0</code> Nucleus sampling parameter <code>frequency_penalty</code> <code>float</code> <code>0.0</code> Penalty for token frequency <code>presence_penalty</code> <code>float</code> <code>0.0</code> Penalty for token presence"},{"location":"getting-started/configuration/#performance-features","title":"Performance Features","text":"Parameter Type Default Description <code>online</code> <code>bool</code> <code>False</code> Enable web search <code>cache_prompt</code> <code>bool</code> <code>True</code> Cache identical prompts <code>max_budget</code> <code>float</code> <code>0.05</code> Maximum cost per request in USD <code>timeout</code> <code>int</code> <code>60</code> Request timeout in seconds"},{"location":"getting-started/configuration/#user-tracking","title":"User Tracking","text":"Parameter Type Default Description <code>user_id</code> <code>str</code> <code>None</code> User identifier for budget tracking <code>client_id</code> <code>str</code> <code>\"default\"</code> Client identifier for usage tracking"},{"location":"getting-started/configuration/#debug-options","title":"Debug Options","text":"Parameter Type Default Description <code>verbose</code> <code>bool</code> <code>False</code> Detailed output <code>logs</code> <code>bool</code> <code>False</code> Enable logging"},{"location":"getting-started/configuration/#configuration-examples","title":"Configuration Examples","text":""},{"location":"getting-started/configuration/#basic-configuration_1","title":"Basic Configuration","text":"<pre><code>config = Request(\n    model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",\n    answer_model=YourResponseModel\n)\n</code></pre>"},{"location":"getting-started/configuration/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>config = Request(\n    # Model settings\n    model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",\n    answer_model=YourResponseModel,\n    temperature=0.5,\n    max_tokens=1000,\n    top_p=0.9,\n    frequency_penalty=0.2,\n    presence_penalty=0.2,\n\n    # Performance features\n    online=True,\n    cache_prompt=True,\n    max_budget=0.1,\n    timeout=120,\n\n    # User tracking\n    user_id=\"user123\",\n    client_id=\"my_app\",\n\n    # Debug options\n    verbose=True,\n    logs=True\n)\n</code></pre>"},{"location":"getting-started/configuration/#configuration-for-production","title":"Configuration for Production","text":"<pre><code>config = Request(\n    model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",\n    answer_model=YourResponseModel,\n    temperature=0.3,  # Lower temperature for more deterministic responses\n    max_tokens=500,\n    cache_prompt=True,  # Enable caching for better performance\n    max_budget=0.05,  # Set a budget limit\n    timeout=30,  # Shorter timeout for production\n    verbose=False,  # Disable verbose output\n    logs=True  # Keep logs enabled for debugging\n)\n</code></pre>"},{"location":"getting-started/configuration/#configuration-for-development","title":"Configuration for Development","text":"<pre><code>config = Request(\n    model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",\n    answer_model=YourResponseModel,\n    temperature=0.7,  # Higher temperature for more creative responses\n    max_tokens=1000,  # More tokens for longer responses\n    cache_prompt=False,  # Disable caching for testing\n    max_budget=0.1,  # Higher budget for testing\n    timeout=60,  # Longer timeout for debugging\n    verbose=True,  # Enable verbose output\n    logs=True  # Enable logs\n)\n</code></pre>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<p>Now that you understand how to configure Pydantic2, check out the Message Handling guide to learn how to work with messages.</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<p>Pydantic2 requires Python 3.7 or later.</p>"},{"location":"getting-started/installation/#installing-with-pip","title":"Installing with pip","text":"<pre><code>pip install pydantic2\n</code></pre>"},{"location":"getting-started/installation/#api-keys","title":"API Keys","text":"<p>Pydantic2 uses OpenRouter as the default model provider. You'll need to set up your API key:</p> <pre><code>export OPENROUTER_API_KEY=your_api_key_here\n</code></pre> <p>You can get an API key from OpenRouter.</p> <p>Alternatively, you can set the API key in your code:</p> <pre><code>import os\nos.environ[\"OPENROUTER_API_KEY\"] = \"your_api_key_here\"\n</code></pre>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<p>You can verify that Pydantic2 is installed correctly by running:</p> <pre><code>import pydantic2\nprint(pydantic2.__version__)\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Once you have Pydantic2 installed, check out the Quick Start guide to learn how to use it.</p>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>This guide will help you get started with Pydantic2 quickly. We'll cover the basics of setting up a client, defining a response model, and generating a response.</p>"},{"location":"getting-started/quick-start/#basic-example","title":"Basic Example","text":"<p>Here's a simple example that demonstrates the core functionality of Pydantic2:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List\nimport json\n\nfrom pydantic2 import LiteLLMClient, Request\n\n\n# Define a custom response model\nclass UserDetail(BaseModel):\n    \"\"\"Model for extracting user details from text.\"\"\"\n    name: str = Field(description=\"The user's name\")\n    age: int = Field(description=\"The user's age\")\n    interests: List[str] = Field(description=\"List of user's interests\")\n\n\n# Create a request configuration\nconfig = Request(\n    model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",  # Model identifier\n    temperature=0.7,                                   # Response randomness\n    max_tokens=500,                                    # Maximum response length\n    answer_model=UserDetail,                           # Pydantic model for responses\n    verbose=True                                       # Show detailed output\n)\n\n# Initialize the client\nclient = LiteLLMClient(config)\n\n# Add a message to the conversation\nclient.msg.add_message_user(\"Describe who is David Copperfield\")\n\n# Generate a response\nresponse: UserDetail = client.generate_response()\n\n# Print the structured response\nprint(json.dumps(response.model_dump(), indent=2))\n</code></pre>"},{"location":"getting-started/quick-start/#step-by-step-explanation","title":"Step-by-Step Explanation","text":""},{"location":"getting-started/quick-start/#1-define-a-response-model","title":"1. Define a Response Model","text":"<p>First, define a Pydantic model that represents the structure of the response you want:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List\n\nclass UserDetail(BaseModel):\n    \"\"\"Model for extracting user details from text.\"\"\"\n    name: str = Field(description=\"The user's name\")\n    age: int = Field(description=\"The user's age\")\n    interests: List[str] = Field(description=\"List of user's interests\")\n</code></pre> <p>The model should inherit from <code>BaseModel</code> and define the fields you want to extract. Each field should have a type annotation and a description.</p>"},{"location":"getting-started/quick-start/#2-configure-the-request","title":"2. Configure the Request","text":"<p>Next, create a <code>Request</code> object with your desired configuration:</p> <pre><code>from pydantic2 import Request\n\nconfig = Request(\n    model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",  # Model identifier\n    temperature=0.7,                                   # Response randomness\n    max_tokens=500,                                    # Maximum response length\n    answer_model=UserDetail,                           # Pydantic model for responses\n    verbose=True                                       # Show detailed output\n)\n</code></pre> <p>Key parameters: - <code>model</code>: The identifier of the LLM model to use - <code>answer_model</code>: Your Pydantic model for structured responses - <code>temperature</code>: Controls randomness (0.0-1.0) - <code>max_tokens</code>: Maximum length of the response - <code>verbose</code>: Whether to show detailed output</p>"},{"location":"getting-started/quick-start/#3-initialize-the-client","title":"3. Initialize the Client","text":"<p>Create a <code>LiteLLMClient</code> with your configuration:</p> <pre><code>from pydantic2 import LiteLLMClient\n\nclient = LiteLLMClient(config)\n</code></pre>"},{"location":"getting-started/quick-start/#4-add-messages","title":"4. Add Messages","text":"<p>Add messages to the conversation:</p> <pre><code>client.msg.add_message_user(\"Describe who is David Copperfield\")\n</code></pre> <p>You can add different types of messages: - <code>add_message_user()</code>: Add a user message - <code>add_message_system()</code>: Add a system message - <code>add_message_assistant()</code>: Add an assistant message - <code>add_message_block()</code>: Add a block message with a tag</p>"},{"location":"getting-started/quick-start/#5-generate-a-response","title":"5. Generate a Response","text":"<p>Generate a response and get it in your structured format:</p> <pre><code>response: UserDetail = client.generate_response()\n</code></pre> <p>The response will be an instance of your Pydantic model.</p>"},{"location":"getting-started/quick-start/#6-use-the-response","title":"6. Use the Response","text":"<p>You can access the fields of the response directly:</p> <pre><code>print(f\"Name: {response.name}\")\nprint(f\"Age: {response.age}\")\nprint(f\"Interests: {', '.join(response.interests)}\")\n</code></pre> <p>Or convert it to a dictionary or JSON:</p> <pre><code># Convert to dictionary\nresponse_dict = response.model_dump()\n\n# Convert to JSON\nresponse_json = response.model_dump_json(indent=2)\nprint(response_json)\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<p>Now that you've seen the basics, check out the Configuration guide to learn more about the available configuration options.</p>"},{"location":"guides/budget-management/","title":"Budget Management","text":"<p>Pydantic2 includes built-in budget management features to help you control costs when working with LLMs. This guide covers how to use these features.</p>"},{"location":"guides/budget-management/#setting-a-budget","title":"Setting a Budget","text":"<p>You can set a budget limit for each request in the <code>Request</code> configuration:</p> <pre><code>from pydantic2 import Request, LiteLLMClient\n\nconfig = Request(\n    model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",\n    answer_model=YourResponseModel,\n    max_budget=0.05,  # Maximum $0.05 USD per request\n    user_id=\"user123\"  # Track budget by user\n)\n\nclient = LiteLLMClient(config)\n</code></pre> <p>The <code>max_budget</code> parameter sets the maximum cost in USD for each request. If a request would exceed this budget, it will be rejected.</p>"},{"location":"guides/budget-management/#user-based-budget-tracking","title":"User-Based Budget Tracking","text":"<p>You can track budgets by user by setting the <code>user_id</code> parameter:</p> <pre><code>config = Request(\n    model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",\n    answer_model=YourResponseModel,\n    max_budget=0.05,\n    user_id=\"user123\"  # Track budget by user\n)\n</code></pre> <p>This allows you to track and limit spending on a per-user basis.</p>"},{"location":"guides/budget-management/#getting-budget-information","title":"Getting Budget Information","text":"<p>You can get information about the current budget:</p> <pre><code>budget_info = client.get_budget_info()\nprint(budget_info)\n</code></pre> <p>The <code>budget_info</code> dictionary contains the following information:</p> <ul> <li><code>max_budget</code>: The maximum budget for the request</li> <li><code>estimated_cost</code>: The estimated cost of the request</li> <li><code>remaining_budget</code>: The remaining budget after the request</li> <li><code>budget_exceeded</code>: Whether the budget would be exceeded</li> </ul>"},{"location":"guides/budget-management/#calculating-costs","title":"Calculating Costs","text":"<p>You can calculate the cost of a request after it has been made:</p> <pre><code>cost = client.calculate_cost()\nprint(f\"Request cost: ${cost:.6f}\")\n</code></pre>"},{"location":"guides/budget-management/#token-counting","title":"Token Counting","text":"<p>You can count the tokens in a prompt before sending it:</p> <pre><code>prompt_tokens = client.count_tokens()\nprint(f\"Prompt token count: {prompt_tokens}\")\n</code></pre> <p>You can also get the maximum number of tokens for the model:</p> <pre><code>max_tokens = client.get_max_tokens_for_model()\nprint(f\"Max tokens for model: {max_tokens}\")\n</code></pre>"},{"location":"guides/budget-management/#usage-tracking","title":"Usage Tracking","text":"<p>You can track usage statistics:</p> <pre><code># Print usage information\nclient.print_usage_info()\n\n# Get usage statistics\nusage_stats = client.usage_tracker.get_usage_stats()\nprint(usage_stats)\n</code></pre>"},{"location":"guides/budget-management/#complete-example","title":"Complete Example","text":"<p>Here's a complete example of using the budget management features:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List\nimport uuid\n\nfrom pydantic2 import LiteLLMClient, Request\n\n\n# Define a custom response model\nclass UserDetail(BaseModel):\n    \"\"\"Model for extracting user details from text.\"\"\"\n    name: str = Field(description=\"The user's name\")\n    age: int = Field(description=\"The user's age\")\n    interests: List[str] = Field(description=\"List of user's interests\")\n\n\ndef main():\n    \"\"\"Example using LiteLLM client with OpenAI and cost tracking.\"\"\"\n    # Generate a unique user_id for the example\n    # In a real application, this would be the user ID from your system\n    user_id = str(uuid.uuid4())\n    print(f\"Using user_id: {user_id}\")\n\n    # Set a budget for the user\n    max_budget = 0.0001  # $0.0001 USD (very small budget)\n    print(f\"Setting budget: ${max_budget} USD (very small budget)\")\n\n    # Create a request configuration\n    config = Request(\n        model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",\n        temperature=0.7,\n        max_tokens=500,\n        max_budget=max_budget,  # Setting a budget limit in USD\n        client_id='demo',\n        user_id=user_id,  # Set user_id for budgeting\n        answer_model=UserDetail,  # The Pydantic model to use for the response\n        verbose=True\n    )\n\n    # Initialize the client\n    client = LiteLLMClient(config)\n\n    client.msg.add_message_user(\"Describe who is David Copperfield\")\n\n    try:\n        # Count tokens in the prompt before sending\n        prompt_tokens = client.count_tokens()\n        print(f\"\\nPrompt token count: {prompt_tokens}\")\n\n        # Get max tokens for the model\n        max_tokens = client.get_max_tokens_for_model()\n        print(f\"Max tokens for model: {max_tokens}\")\n\n        # Get budget information\n        budget_info = client.get_budget_info()\n        print(\"\\n=== Budget Information ===\")\n        for key, value in budget_info.items():\n            if isinstance(value, float):\n                print(f\"{key}: ${value:.6f}\")\n            else:\n                print(f\"{key}: {value}\")\n\n        # Generate a response\n        response: UserDetail = client.generate_response()\n\n        # Print the structured response\n        print(\"\\nStructured Response:\")\n        print(response.model_dump())\n\n        # Calculate cost after response\n        cost = client.calculate_cost()\n        print(f\"\\nRequest cost: ${cost:.6f}\")\n\n        # Print usage information\n        client.print_usage_info()\n\n        # Get usage statistics\n        usage_stats = client.usage_tracker.get_usage_stats()\n        print(f\"Usage stats: {usage_stats}\")\n\n    except Exception as e:\n        print(f\"Error: {e}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"guides/budget-management/#best-practices","title":"Best Practices","text":"<p>Here are some best practices for budget management:</p> <ol> <li>Set reasonable budgets: Start with a small budget and increase it as needed.</li> <li>Track usage by user: Use the <code>user_id</code> parameter to track usage by user.</li> <li>Monitor costs: Regularly check usage statistics to monitor costs.</li> <li>Use token counting: Count tokens before sending requests to estimate costs.</li> <li>Handle budget exceptions: Catch exceptions when the budget is exceeded.</li> </ol>"},{"location":"guides/budget-management/#next-steps","title":"Next Steps","text":"<p>Now that you understand how to manage your budget, check out the Structured Responses guide to learn how to work with structured responses.</p>"},{"location":"guides/message-handling/","title":"Message Handling","text":"<p>Pydantic2 provides a flexible message handling system through the <code>MessageHandler</code> class. This guide covers how to work with messages in Pydantic2.</p>"},{"location":"guides/message-handling/#message-types","title":"Message Types","text":"<p>Pydantic2 supports four types of messages:</p> <ol> <li>System Messages: Set the AI's behavior and context</li> <li>User Messages: Send queries or inputs</li> <li>Assistant Messages: Add AI responses or context</li> <li>Block Messages: Add structured data with tags</li> </ol>"},{"location":"guides/message-handling/#adding-messages","title":"Adding Messages","text":""},{"location":"guides/message-handling/#system-messages","title":"System Messages","text":"<p>System messages are used to set the AI's behavior and context:</p> <pre><code># Add a simple system message\nclient.msg.add_message_system(\"You are a helpful assistant.\")\n\n# Add a structured system message\nclient.msg.add_message_system({\n    \"role\": \"expert\",\n    \"expertise\": [\"python\", \"data analysis\"]\n})\n</code></pre>"},{"location":"guides/message-handling/#user-messages","title":"User Messages","text":"<p>User messages are used to send queries or inputs:</p> <pre><code># Add a simple user message\nclient.msg.add_message_user(\"Analyze this data\")\n\n# Add a structured user message\nclient.msg.add_message_user({\n    \"query\": \"analyze trends\",\n    \"metrics\": [\"users\", \"revenue\"]\n})\n</code></pre>"},{"location":"guides/message-handling/#assistant-messages","title":"Assistant Messages","text":"<p>Assistant messages are used to add AI responses or context:</p> <pre><code># Add a simple assistant message\nclient.msg.add_message_assistant(\"Based on the data...\")\n\n# Add a structured assistant message\nclient.msg.add_message_assistant([\n    \"Point 1: Growth is steady\",\n    \"Point 2: Conversion improved\"\n])\n</code></pre>"},{"location":"guides/message-handling/#block-messages","title":"Block Messages","text":"<p>Block messages are used to add structured data with tags:</p> <pre><code># Add a code block\nclient.msg.add_message_block(\"CODE\", \"\"\"\ndef hello(): print(\"Hello, World!\")\n\"\"\")\n\n# Add a data block\nclient.msg.add_message_block(\"DATA\", {\n    \"users\": [{\"id\": 1, \"name\": \"John\"}, {\"id\": 2, \"name\": \"Jane\"}],\n    \"metrics\": {\"total\": 100, \"active\": 80}\n})\n</code></pre>"},{"location":"guides/message-handling/#supported-data-types","title":"Supported Data Types","text":"<p>Pydantic2 automatically formats various data types:</p> <ul> <li>Basic types (str, int, float, bool)</li> <li>Collections (lists, dicts)</li> <li>Pandas DataFrames</li> <li>Pydantic models</li> <li>Dataclasses</li> <li>Custom objects with str</li> <li>JSON-serializable objects</li> </ul>"},{"location":"guides/message-handling/#example-with-pandas-dataframe","title":"Example with Pandas DataFrame","text":"<pre><code>import pandas as pd\n\n# Create a DataFrame\ndf = pd.DataFrame({\n    \"name\": [\"John\", \"Jane\", \"Bob\"],\n    \"age\": [25, 30, 35],\n    \"active\": [True, False, True]\n})\n\n# Add it as a block message\nclient.msg.add_message_block(\"DATA\", df)\n</code></pre>"},{"location":"guides/message-handling/#example-with-pydantic-model","title":"Example with Pydantic Model","text":"<pre><code>from pydantic import BaseModel\nfrom typing import List\n\nclass User(BaseModel):\n    name: str\n    age: int\n    active: bool\n\n# Create a list of users\nusers = [\n    User(name=\"John\", age=25, active=True),\n    User(name=\"Jane\", age=30, active=False),\n    User(name=\"Bob\", age=35, active=True)\n]\n\n# Add it as a block message\nclient.msg.add_message_block(\"USERS\", users)\n</code></pre>"},{"location":"guides/message-handling/#getting-messages","title":"Getting Messages","text":"<p>You can get all messages in the conversation:</p> <pre><code>messages = client.msg.get_messages()\n</code></pre>"},{"location":"guides/message-handling/#clearing-messages","title":"Clearing Messages","text":"<p>You can clear all messages in the conversation:</p> <pre><code>client.msg.clear_messages()\n</code></pre>"},{"location":"guides/message-handling/#complete-example","title":"Complete Example","text":"<p>Here's a complete example of using the message handling system:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List\nimport pandas as pd\n\nfrom pydantic2 import LiteLLMClient, Request\n\n\n# Define a custom response model\nclass AnalysisResult(BaseModel):\n    \"\"\"Model for data analysis results.\"\"\"\n    summary: str = Field(description=\"Summary of the analysis\")\n    trends: List[str] = Field(description=\"Identified trends\")\n    recommendations: List[str] = Field(description=\"Recommendations based on the analysis\")\n\n\n# Create a request configuration\nconfig = Request(\n    model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",\n    answer_model=AnalysisResult,\n    temperature=0.7,\n    max_tokens=500\n)\n\n# Initialize the client\nclient = LiteLLMClient(config)\n\n# Add system message\nclient.msg.add_message_system(\"You are a data analysis expert.\")\n\n# Create a DataFrame\ndf = pd.DataFrame({\n    \"date\": [\"2023-01-01\", \"2023-01-02\", \"2023-01-03\", \"2023-01-04\", \"2023-01-05\"],\n    \"users\": [100, 120, 150, 130, 160],\n    \"revenue\": [500, 600, 750, 650, 800]\n})\n\n# Add user message\nclient.msg.add_message_user(\"Analyze this data and identify trends\")\n\n# Add data block\nclient.msg.add_message_block(\"DATA\", df)\n\n# Generate a response\nresponse: AnalysisResult = client.generate_response()\n\n# Print the structured response\nprint(f\"Summary: {response.summary}\")\nprint(\"\\nTrends:\")\nfor trend in response.trends:\n    print(f\"- {trend}\")\nprint(\"\\nRecommendations:\")\nfor recommendation in response.recommendations:\n    print(f\"- {recommendation}\")\n</code></pre>"},{"location":"guides/message-handling/#next-steps","title":"Next Steps","text":"<p>Now that you understand how to work with messages, check out the Budget Management guide to learn how to manage your budget.</p>"},{"location":"guides/structured-responses/","title":"Structured Responses","text":"<p>One of the key features of Pydantic2 is the ability to get structured responses from LLMs using Pydantic models. This guide covers how to define and use structured response models.</p>"},{"location":"guides/structured-responses/#defining-response-models","title":"Defining Response Models","text":"<p>Response models are Pydantic models that define the structure of the response you want from the LLM. Here's a simple example:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List, Optional\n\nclass MovieReview(BaseModel):\n    \"\"\"Model for a movie review.\"\"\"\n    title: str = Field(description=\"The title of the movie\")\n    director: str = Field(description=\"The director of the movie\")\n    year: int = Field(description=\"The year the movie was released\")\n    rating: float = Field(description=\"The rating of the movie (0-10)\")\n    pros: List[str] = Field(description=\"The pros of the movie\")\n    cons: List[str] = Field(description=\"The cons of the movie\")\n    summary: str = Field(description=\"A summary of the review\")\n    recommendation: Optional[bool] = Field(None, description=\"Whether the movie is recommended\")\n</code></pre> <p>Each field in the model should have: - A type annotation - A description (using <code>Field</code>) - Optional default values</p>"},{"location":"guides/structured-responses/#using-response-models","title":"Using Response Models","text":"<p>To use a response model, pass it to the <code>Request</code> configuration:</p> <pre><code>from pydantic2 import Request, LiteLLMClient\n\nconfig = Request(\n    model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",\n    answer_model=MovieReview,  # Your response model\n    temperature=0.7,\n    max_tokens=500\n)\n\nclient = LiteLLMClient(config)\n</code></pre> <p>Then, when you generate a response, it will be returned as an instance of your model:</p> <pre><code>client.msg.add_message_user(\"Review the movie 'The Shawshank Redemption'\")\n\nresponse: MovieReview = client.generate_response()\n</code></pre>"},{"location":"guides/structured-responses/#accessing-response-data","title":"Accessing Response Data","text":"<p>You can access the fields of the response directly:</p> <pre><code>print(f\"Title: {response.title}\")\nprint(f\"Director: {response.director}\")\nprint(f\"Year: {response.year}\")\nprint(f\"Rating: {response.rating}/10\")\nprint(\"\\nPros:\")\nfor pro in response.pros:\n    print(f\"- {pro}\")\nprint(\"\\nCons:\")\nfor con in response.cons:\n    print(f\"- {con}\")\nprint(f\"\\nSummary: {response.summary}\")\nif response.recommendation is not None:\n    print(f\"Recommendation: {'Recommended' if response.recommendation else 'Not recommended'}\")\n</code></pre> <p>You can also convert the response to a dictionary or JSON:</p> <pre><code># Convert to dictionary\nresponse_dict = response.model_dump()\n\n# Convert to JSON\nresponse_json = response.model_dump_json(indent=2)\nprint(response_json)\n</code></pre>"},{"location":"guides/structured-responses/#advanced-response-models","title":"Advanced Response Models","text":"<p>You can create more complex response models using nested models, enums, and more.</p>"},{"location":"guides/structured-responses/#nested-models","title":"Nested Models","text":"<pre><code>from pydantic import BaseModel, Field\nfrom typing import List, Optional\nfrom enum import Enum\n\nclass Genre(str, Enum):\n    ACTION = \"action\"\n    COMEDY = \"comedy\"\n    DRAMA = \"drama\"\n    HORROR = \"horror\"\n    SCIFI = \"sci-fi\"\n    THRILLER = \"thriller\"\n    OTHER = \"other\"\n\nclass Person(BaseModel):\n    name: str = Field(description=\"The name of the person\")\n    role: str = Field(description=\"The role of the person\")\n\nclass MovieDetails(BaseModel):\n    runtime: int = Field(description=\"The runtime of the movie in minutes\")\n    budget: Optional[float] = Field(None, description=\"The budget of the movie in millions of dollars\")\n    box_office: Optional[float] = Field(None, description=\"The box office of the movie in millions of dollars\")\n\nclass MovieReview(BaseModel):\n    \"\"\"Model for a detailed movie review.\"\"\"\n    title: str = Field(description=\"The title of the movie\")\n    director: Person = Field(description=\"The director of the movie\")\n    cast: List[Person] = Field(description=\"The main cast of the movie\")\n    year: int = Field(description=\"The year the movie was released\")\n    genres: List[Genre] = Field(description=\"The genres of the movie\")\n    rating: float = Field(description=\"The rating of the movie (0-10)\")\n    details: MovieDetails = Field(description=\"Additional details about the movie\")\n    pros: List[str] = Field(description=\"The pros of the movie\")\n    cons: List[str] = Field(description=\"The cons of the movie\")\n    summary: str = Field(description=\"A summary of the review\")\n    recommendation: Optional[bool] = Field(None, description=\"Whether the movie is recommended\")\n</code></pre>"},{"location":"guides/structured-responses/#best-practices","title":"Best Practices","text":"<p>Here are some best practices for defining response models:</p> <ol> <li>Keep it simple: Start with simple models and add complexity as needed.</li> <li>Use descriptive field names: Field names should be clear and descriptive.</li> <li>Add detailed descriptions: Each field should have a detailed description.</li> <li>Use appropriate types: Use the most appropriate type for each field.</li> <li>Set default values: Use default values for optional fields.</li> <li>Use nested models: Use nested models for complex structures.</li> <li>Use enums: Use enums for fields with a fixed set of values.</li> <li>Add docstrings: Add docstrings to models for better documentation.</li> </ol>"},{"location":"guides/structured-responses/#complete-example","title":"Complete Example","text":"<p>Here's a complete example of using structured responses:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List, Optional\nfrom enum import Enum\n\nfrom pydantic2 import LiteLLMClient, Request\n\n\nclass Genre(str, Enum):\n    ACTION = \"action\"\n    COMEDY = \"comedy\"\n    DRAMA = \"drama\"\n    HORROR = \"horror\"\n    SCIFI = \"sci-fi\"\n    THRILLER = \"thriller\"\n    OTHER = \"other\"\n\n\nclass Person(BaseModel):\n    name: str = Field(description=\"The name of the person\")\n    role: str = Field(description=\"The role of the person\")\n\n\nclass MovieDetails(BaseModel):\n    runtime: int = Field(description=\"The runtime of the movie in minutes\")\n    budget: Optional[float] = Field(None, description=\"The budget of the movie in millions of dollars\")\n    box_office: Optional[float] = Field(None, description=\"The box office of the movie in millions of dollars\")\n\n\nclass MovieReview(BaseModel):\n    \"\"\"Model for a detailed movie review.\"\"\"\n    title: str = Field(description=\"The title of the movie\")\n    director: Person = Field(description=\"The director of the movie\")\n    cast: List[Person] = Field(description=\"The main cast of the movie\")\n    year: int = Field(description=\"The year the movie was released\")\n    genres: List[Genre] = Field(description=\"The genres of the movie\")\n    rating: float = Field(description=\"The rating of the movie (0-10)\")\n    details: MovieDetails = Field(description=\"Additional details about the movie\")\n    pros: List[str] = Field(description=\"The pros of the movie\")\n    cons: List[str] = Field(description=\"The cons of the movie\")\n    summary: str = Field(description=\"A summary of the review\")\n    recommendation: Optional[bool] = Field(None, description=\"Whether the movie is recommended\")\n\n\ndef main():\n    \"\"\"Example using structured responses.\"\"\"\n    # Create a request configuration\n    config = Request(\n        model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",\n        answer_model=MovieReview,\n        temperature=0.7,\n        max_tokens=1000,\n        verbose=True\n    )\n\n    # Initialize the client\n    client = LiteLLMClient(config)\n\n    # Add a system message\n    client.msg.add_message_system(\"You are a movie critic with deep knowledge of film history and techniques.\")\n\n    # Add a user message\n    client.msg.add_message_user(\"Review the movie 'The Shawshank Redemption'\")\n\n    # Generate a response\n    response: MovieReview = client.generate_response()\n\n    # Print the structured response\n    print(f\"Title: {response.title}\")\n    print(f\"Director: {response.director.name} ({response.director.role})\")\n    print(f\"Year: {response.year}\")\n    print(f\"Genres: {', '.join(genre.value for genre in response.genres)}\")\n    print(f\"Rating: {response.rating}/10\")\n\n    print(\"\\nCast:\")\n    for person in response.cast:\n        print(f\"- {person.name} ({person.role})\")\n\n    print(\"\\nDetails:\")\n    print(f\"- Runtime: {response.details.runtime} minutes\")\n    if response.details.budget:\n        print(f\"- Budget: ${response.details.budget} million\")\n    if response.details.box_office:\n        print(f\"- Box Office: ${response.details.box_office} million\")\n\n    print(\"\\nPros:\")\n    for pro in response.pros:\n        print(f\"- {pro}\")\n\n    print(\"\\nCons:\")\n    for con in response.cons:\n        print(f\"- {con}\")\n\n    print(f\"\\nSummary: {response.summary}\")\n\n    if response.recommendation is not None:\n        print(f\"Recommendation: {'Recommended' if response.recommendation else 'Not recommended'}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"guides/structured-responses/#next-steps","title":"Next Steps","text":"<p>Now that you understand how to work with structured responses, check out the Usage Tracking guide to learn how to track usage.</p>"},{"location":"guides/usage-tracking/","title":"Usage Tracking","text":"<p>Pydantic2 provides detailed usage tracking features to help you monitor and manage your LLM usage. This guide covers how to use these features.</p>"},{"location":"guides/usage-tracking/#basic-usage-tracking","title":"Basic Usage Tracking","text":"<p>You can track usage statistics using the <code>usage_tracker</code> property of the <code>LiteLLMClient</code>:</p> <pre><code>from pydantic2 import LiteLLMClient, Request\n\nconfig = Request(\n    model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",\n    answer_model=YourResponseModel,\n    client_id=\"my_application\"  # Important for tracking\n)\n\nclient = LiteLLMClient(config)\n\n# After making some requests, get usage statistics\nusage_stats = client.usage_tracker.get_usage_stats()\nprint(usage_stats)\n</code></pre> <p>The <code>client_id</code> parameter is important for tracking usage across multiple requests.</p>"},{"location":"guides/usage-tracking/#printing-usage-information","title":"Printing Usage Information","text":"<p>You can print usage information after a request:</p> <pre><code>client.print_usage_info()\n</code></pre> <p>This will print information about the request, including: - Model used - Token count - Cost - Response time</p>"},{"location":"guides/usage-tracking/#structured-usage-tracking","title":"Structured Usage Tracking","text":"<p>Pydantic2 provides structured usage tracking with Pydantic models for easy integration:</p> <pre><code>from pydantic2 import LiteLLMClient, Request\nimport json\n\n# Initialize client with client_id for tracking\nconfig = Request(\n    model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",\n    answer_model=YourResponseModel,\n    client_id=\"my_application\",  # Important for tracking\n    verbose=True\n)\nclient = LiteLLMClient(config)\n\n# After making some requests, get detailed usage data\nusage_data = client.usage_tracker.get_client_usage_data()\n\n# Access structured data with type hints\nprint(f\"Total requests: {usage_data.total_requests}\")\nprint(f\"Successful requests: {usage_data.successful_requests}\")\nprint(f\"Total cost: ${usage_data.total_cost:.6f}\")\n\n# Print models used\nprint(\"\\nModels used:\")\nfor model in usage_data.models_used:\n    print(f\"- {model.model_name}: {model.total_input_tokens + model.total_output_tokens} tokens, ${model.total_cost:.6f}\")\n\n# Print recent requests\nprint(\"\\nRecent requests:\")\nfor req in usage_data.recent_requests:\n    print(f\"- {req.timestamp}: {req.model_name}, Status: {req.status}, Cost: ${req.total_cost:.6f}\")\n\n# Convert to JSON for API responses or storage\nusage_json = usage_data.model_dump_json(indent=2)\nprint(f\"\\nJSON representation:\\n{usage_json}\")\n\n# Get usage for a different client\nother_client_usage = client.usage_tracker.get_client_usage_data(client_id=\"other_app\")\n</code></pre>"},{"location":"guides/usage-tracking/#detailed-request-tracking","title":"Detailed Request Tracking","text":"<p>You can retrieve detailed information about specific requests:</p> <pre><code>from pydantic2 import LiteLLMClient, Request\n\n# Initialize client\nconfig = Request(\n    model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",\n    answer_model=YourResponseModel,\n    client_id=\"my_application\",\n    verbose=True\n)\nclient = LiteLLMClient(config)\n\n# Make a request and get its ID\nresponse = client.generate_response()\nrequest_id = client.meta.request_id\nprint(f\"Current request_id: {request_id}\")\n\n# Get detailed information about a specific request\nrequest_details = client.usage_tracker.get_request_details(request_id=request_id)\n\nif request_details:\n    # Access structured data with type hints\n    print(f\"Request ID: {request_details.request_id}\")\n    print(f\"Timestamp: {request_details.timestamp}\")\n    print(f\"Model: {request_details.model_name} ({request_details.model_provider})\")\n    print(f\"Status: {request_details.status}\")\n    print(f\"Tokens: {request_details.input_tokens} input, {request_details.output_tokens} output\")\n    print(f\"Cost: ${request_details.total_cost:.6f}\")\n\n    # Convert to JSON for API responses or storage\n    details_json = request_details.model_dump_json(indent=2)\n    print(f\"\\nJSON representation:\\n{details_json}\")\nelse:\n    print(\"No request details found\")\n\n# You can also retrieve details for any request ID\nhistorical_request_id = \"5f01f28b-1f9a-427a-ae69-b6c4842c0ee3\"\nhistorical_details = client.usage_tracker.get_request_details(request_id=historical_request_id)\n</code></pre>"},{"location":"guides/usage-tracking/#building-a-usage-dashboard","title":"Building a Usage Dashboard","text":"<p>You can use the structured usage data to build a dashboard for monitoring usage:</p> <pre><code>from fastapi import FastAPI, Depends, HTTPException, Security, Query\nfrom fastapi.security import APIKeyHeader\nfrom typing import Optional, List\nimport uvicorn\n\nfrom pydantic2 import LiteLLMClient, Request\nfrom pydantic2.client.usage.usage_class import ClientUsageData, RequestDetails\n\n# Initialize a shared client for tracking usage\nconfig = Request(\n    model=\"openrouter/openai/gpt-4o-mini-2024-07-18\",\n    answer_model=None,  # Not needed for tracking only\n    client_id=\"dashboard\",\n    verbose=False\n)\nclient = LiteLLMClient(config)\n\n# Create a FastAPI app\napp = FastAPI(title=\"LLM Usage Dashboard\")\n\n# Security - API key authentication\nAPI_KEY = \"your-secret-api-key\"  # In production, use environment variables\napi_key_header = APIKeyHeader(name=\"X-API-Key\")\n\ndef verify_api_key(api_key: str = Security(api_key_header)):\n    if api_key != API_KEY:\n        raise HTTPException(status_code=403, detail=\"Invalid API key\")\n    return api_key\n\n# Endpoint for getting a summary of usage across all clients\n@app.get(\"/usage/summary\", response_model=dict)\ndef get_usage_summary(api_key: str = Depends(verify_api_key)):\n    \"\"\"Get a summary of usage across all clients.\"\"\"\n    # Get all clients\n    clients = [\"client1\", \"client2\", \"dashboard\"]  # In production, get this from your database\n\n    total_cost = 0.0\n    total_tokens = 0\n    client_data = {}\n\n    for client_id in clients:\n        usage = client.usage_tracker.get_client_usage_data(client_id=client_id)\n        total_cost += usage.total_cost\n        total_tokens += usage.total_input_tokens + usage.total_output_tokens\n        client_data[client_id] = {\n            \"requests\": usage.total_requests,\n            \"cost\": usage.total_cost,\n            \"tokens\": usage.total_input_tokens + usage.total_output_tokens\n        }\n\n    return {\n        \"total_cost\": total_cost,\n        \"total_tokens\": total_tokens,\n        \"clients\": client_data\n    }\n\n# Endpoint for getting detailed usage for a specific client\n@app.get(\"/usage/client/{client_id}\", response_model=ClientUsageData)\ndef get_client_usage(\n    client_id: str,\n    limit: Optional[int] = Query(10, description=\"Limit the number of recent requests\"),\n    api_key: str = Depends(verify_api_key)\n):\n    \"\"\"Get detailed usage for a specific client.\"\"\"\n    usage = client.usage_tracker.get_client_usage_data(client_id=client_id)\n\n    # Limit the number of recent requests\n    if limit and limit &lt; len(usage.recent_requests):\n        usage.recent_requests = usage.recent_requests[:limit]\n\n    return usage\n\n# Endpoint for getting usage by model\n@app.get(\"/usage/models\", response_model=List[dict])\ndef get_model_usage(api_key: str = Depends(verify_api_key)):\n    \"\"\"Get usage broken down by model.\"\"\"\n    # Get all clients\n    clients = [\"client1\", \"client2\", \"dashboard\"]  # In production, get this from your database\n\n    model_data = {}\n\n    for client_id in clients:\n        usage = client.usage_tracker.get_client_usage_data(client_id=client_id)\n\n        for model in usage.models_used:\n            if model.model_name not in model_data:\n                model_data[model.model_name] = {\n                    \"model_name\": model.model_name,\n                    \"model_provider\": model.model_provider,\n                    \"total_tokens\": 0,\n                    \"total_cost\": 0.0,\n                    \"clients\": []\n                }\n\n            model_data[model.model_name][\"total_tokens\"] += model.total_input_tokens + model.total_output_tokens\n            model_data[model.model_name][\"total_cost\"] += model.total_cost\n\n            if client_id not in model_data[model.model_name][\"clients\"]:\n                model_data[model.model_name][\"clients\"].append(client_id)\n\n    return list(model_data.values())\n\n# Run the app\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</code></pre>"},{"location":"guides/usage-tracking/#key-features","title":"Key Features","text":"<p>The usage tracking system provides several key features:</p> <ol> <li>Structured Data: All usage data is returned as typed Pydantic models, ensuring:</li> <li>Strict typing and data validation</li> <li>Autodocumentation through type annotations and field descriptions</li> <li> <p>IDE integration for autocompletion and hints</p> </li> <li> <p>Detailed Statistics: The system provides comprehensive information on:</p> </li> <li>Total number of requests (successful and failed)</li> <li>Total number of tokens (input and output)</li> <li> <p>Overall usage cost</p> </li> <li> <p>Model Breakdown: The system offers separate statistics for each model used:</p> </li> <li>Token count per model</li> <li>Usage cost per model</li> <li> <p>Last usage time</p> </li> <li> <p>Request History: The system includes information on recent requests:</p> </li> <li>Timestamps</li> <li>Execution status</li> <li>Cost of each request</li> <li> <p>Error messages (if any)</p> </li> <li> <p>Easy Serialization: All data can be easily converted to JSON for:</p> </li> <li>API responses</li> <li>Database storage</li> <li> <p>Display in web interfaces</p> </li> <li> <p>Type Safety: All fields have strict typing, helping to avoid errors when working with data.</p> </li> </ol>"},{"location":"guides/usage-tracking/#next-steps","title":"Next Steps","text":"<p>Now that you understand how to track usage, check out the Examples section to see more examples of using Pydantic2.</p>"}]}